{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion Attack   \n",
    "\n",
    "**Goal:** Give a controlled environment to perform an inversion attack and identify what data is in the training set. Should be tested with and without Differential Privacy in the model. Based on: https://arxiv.org/pdf/1610.05820v2.pdf    \n",
    "**TODO:** Refactor to be more generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_privacy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import (\n",
    "    compute_rdp,\n",
    "    get_privacy_spent,\n",
    ")\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-generated csv file (separated by semi-colons)\n",
    "df = pd.read_csv(\"student-mat.csv\", sep=\";\")\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configue the columns to use, can omit some but we are using all of them\n",
    "y_actual = df.G3\n",
    "# Any columns to be removed\n",
    "exclude_var = ['G3']#,'school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet',\n",
    "              #'romantic','goout']\n",
    "# Remove all specified columns\n",
    "df = df.drop(columns=exclude_var)\n",
    "# Encode the values appropriately\n",
    "df_encode = pd.get_dummies(df)\n",
    "print(list(df_encode))\n",
    "# Define the variable to be used later, can be refactored\n",
    "X_actual = df_encode\n",
    "# Bin the grades to be a binary problem\n",
    "ybin = np.asarray(y_actual)\n",
    "ybin[ybin < 10] = 0\n",
    "ybin[ybin >= 10] = 1\n",
    "print(ybin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the data\n",
    "X_actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# min-max normalization\n",
    "X_actual_min = np.min(X_actual)\n",
    "X_actual_max = np.max(X_actual)\n",
    "#X_actual = (X_actual - np.min(X_actual)) / (np.max(X_actual) - np.min(X_actual)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_data(data, actual_min, actual_max):\n",
    "    \"\"\"\n",
    "    Normalizes data within numpy arrays.\n",
    "    \"\"\"\n",
    "    return (data - actual_min) / (actual_max - actual_min).values\n",
    "\n",
    "def normalize_individual_data(data, actual_min, actual_max):\n",
    "    \"\"\"\n",
    "    Normalizes individual values, outside of numpy arrays.\n",
    "    \"\"\"\n",
    "    return (data - actual_min) / (actual_max - actual_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize the data used\n",
    "X_actual = normalize_data(X_actual, X_actual_min, X_actual_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_actual, ybin, test_size=0.2, random_state=0)\n",
    "\n",
    "# get validation dataset\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Get the shape\n",
    "training_size = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from here https://github.com/VectorInstitute/PETs-Bootcamp/blob/main/DP_TensorFlowPrivacy/TFP_HeartDisease_KerasMLP_GridSearch.ipynb\n",
    "def train(\n",
    "    noise_multiplier,\n",
    "    l2_norm_clip,\n",
    "    batch_size,\n",
    "    microbatches,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    dpsgd=True,\n",
    "    learning_rate=0.1,\n",
    "    epochs=150,\n",
    "    model_dir=None,\n",
    "    print_outputs=True,\n",
    "    verbose=1,\n",
    "):\n",
    "\n",
    "    if dpsgd and batch_size % microbatches != 0:\n",
    "        raise ValueError(\"Number of microbatches should divide evenly batch_size\")\n",
    "\n",
    "    # Define a sequential Keras model\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(40, input_dim=58, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "            #tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            # CHANGED: V2 for mia attack we need logit output\n",
    "            tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if dpsgd:\n",
    "        optimizer = DPKerasSGDOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            #num_microbatches=microbatches, # COMMENTED OUT\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "        # CHANGED: Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE, from_logits=True) #V2\n",
    "\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        # CHANGED\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(from_logits=True) #V2\n",
    "\n",
    "    # F.MIA'S ATTACK USE THESE PARAMETERS\n",
    "    '''\n",
    "    # specify parameters\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    '''\n",
    "\n",
    "    # Compile model with Keras\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    display(model.summary())\n",
    "\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1\n",
    "    )\n",
    "\n",
    "    # Train model with Keras\n",
    "    history=model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        callbacks=[tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    # plot accuracy for the first model\n",
    "    plt.plot(history.history['accuracy'], label='acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.legend();\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    score_train = model.evaluate(x_train, y_train, verbose=verbose)\n",
    "    score_valid = model.evaluate(x_valid, y_valid, verbose=verbose)\n",
    "    score_test = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # Compute the privacy budget expended.\n",
    "    # // is integer division\n",
    "    if dpsgd:\n",
    "        eps = compute_epsilon(\n",
    "            epochs * training_size // batch_size,\n",
    "            training_size=training_size,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        eps = \"non-private SGD\"\n",
    "\n",
    "    if print_outputs:\n",
    "        print(\n",
    "            \"\\nhyperparamters: learning rate = \"\n",
    "            + str(learning_rate)\n",
    "            + \", noise_multiplier = \"\n",
    "            + str(noise_multiplier)\n",
    "            + \", l2_norm_clip = \"\n",
    "            + str(l2_norm_clip)\n",
    "            + \", epochs = \"\n",
    "            + str(epochs)\n",
    "            + \", batch_size = \"\n",
    "            + str(batch_size)\n",
    "            + \", microbatches = \"\n",
    "            + str(microbatches)\n",
    "        )\n",
    "\n",
    "        print(\"  training loss: %.2f\" % score_train[0])\n",
    "        print(\"  training accuracy: %.2f\" % score_train[1])\n",
    "\n",
    "        print(\"  validation loss: %.2f\" % score_valid[0])\n",
    "        print(\"  validation accuracy: %.2f\" % score_valid[1])\n",
    "\n",
    "        print(\"  test loss: %.2f\" % score_test[0])\n",
    "        print(\"  test accuracy: %.2f\" % score_test[1])\n",
    "        \n",
    "        Y_pred = model.predict(x_test)\n",
    "        Y_pred = np.argmax(Y_pred, axis=1)#np.where(Y_pred > 0.5, 1,0)#np.argmax(Y_pred,axis=1)\n",
    "        print(classification_report(y_test, Y_pred))   \n",
    "\n",
    "        if dpsgd:\n",
    "            print(\"For delta=0.00413223, the current epsilon is: %.2f\" % eps)\n",
    "        else:\n",
    "            print(\"Trained with vanilla non-private SGD optimizer\")\n",
    "\n",
    "    return score_train, score_valid, score_test, eps, weights, Y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_epsilon(steps, training_size, noise_multiplier, batch_size):\n",
    "    \"\"\"\n",
    "    Computes epsilon value for given hyperparameters.\n",
    "\n",
    "    Parameters required:\n",
    "      steps: Number of steps the optimizer takes over the training data\n",
    "             steps = FLAGS.epochs * training_size// FLAGS.batch_size\n",
    "\n",
    "      Noise multiplier:\n",
    "          the amount of noise sampled and added to gradients during training\n",
    "    \"\"\"\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    \"\"\" \n",
    "  Delta: for (epsilon, delta)-DP\n",
    "    Delta bounds the probability of our privacy guarantee not holding.  \n",
    "    rule of thumb for delta is to set it to less than the inverse of the training data size\n",
    "    so I opted for it to equal to 1.1*training size\n",
    "  \"\"\"\n",
    "    training_delta = 1 / (training_size * 1.1)\n",
    "\n",
    "    \"\"\"\n",
    "  We need to define a list of orders, at which the Rényi divergence will be computed\n",
    "  if you want epsilon between 1-10 and your delta is fixed\n",
    "  your orders must cover the range between 1+ln(1/delta)/10 and 1+ln(1/delta)/1 \n",
    "  \"\"\"\n",
    "    orders = np.linspace(\n",
    "        1 + math.log(1.0 / training_delta) / 10,\n",
    "        1 + math.log(1.0 / training_delta) / 1,\n",
    "        num=100,\n",
    "    )\n",
    "\n",
    "    \"\"\" \n",
    "  Sampling ratio q:\n",
    "    the probability of an individual training point being included in a minibatch\n",
    "    sampling_probability = FLAGS.batch_size / training_size\n",
    "  \"\"\"\n",
    "    sampling_probability = batch_size / training_size\n",
    "\n",
    "    \"\"\" \n",
    "  compute Renyi Differential Privacy, a generalization of pure differential privacy\n",
    "  RDP is well suited to analyze DP guarantees provided by sampling followed by Gaussian noise addition, \n",
    "  which is how gradients are randomized in the TFP implementation of the DP-SGD optimizer.\n",
    "  \"\"\"\n",
    "    rdp = compute_rdp(\n",
    "        q=sampling_probability,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        steps=steps,\n",
    "        orders=orders,\n",
    "    )\n",
    "\n",
    "    return get_privacy_spent(orders, rdp, target_delta=training_delta)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a model, keeping this as the model to be attacked\n",
    "training_outputs = train(\n",
    "    noise_multiplier=1,\n",
    "    l2_norm_clip=1,\n",
    "    batch_size=22,\n",
    "    microbatches=11,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    dpsgd=False,\n",
    "    learning_rate=0.01,\n",
    "    epochs=200,\n",
    "    model_dir=None,\n",
    "    print_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the target model\n",
    "model = training_outputs[-1]\n",
    "# Get the predictions for the\n",
    "preds = model.predict(x_train)\n",
    "preds_full = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNTHETIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StudentRecord:\n",
    "    \"\"\"\n",
    "    Very poorly programmed class for a student record in the dataset.\n",
    "    Can be refactored to avoid hardcoding each value in the class.\n",
    "    \"\"\"\n",
    "    def __init__(self, y_label, actual_min, actual_max):\n",
    "        \"\"\"\n",
    "        Given a y label, and the min/max normalization values,\n",
    "        generates a sample Student when initialized.\n",
    "        \"\"\"\n",
    "        self.age = 15\n",
    "        self.m_edu = 0\n",
    "        self.f_edu = 0\n",
    "        self.travel_time = 1\n",
    "        self.study_time = 1\n",
    "        self.failures = 1\n",
    "        self.fam_rel = 1\n",
    "        self.free_time = 1\n",
    "        self.go_out = 1\n",
    "        self.d_alc = 1\n",
    "        self.w_alc = 1\n",
    "        self.health = 1\n",
    "        self.absences = 0\n",
    "        self.g1 = 0\n",
    "        self.g2 = 0\n",
    "        self.school_GP = 0\n",
    "        self.school_MS = 0\n",
    "        self.sex_F = 0\n",
    "        self.sex_M = 0\n",
    "        self.address_R = 0\n",
    "        self.address_U = 0\n",
    "        self.fam_size_GT3 = 0\n",
    "        self.fam_size_LE3 = 0\n",
    "        self.p_status_a = 0\n",
    "        self.p_status_t = 0\n",
    "        self.m_job_at_home = 0\n",
    "        self.m_job_health = 0\n",
    "        self.m_job_other = 0\n",
    "        self.m_job_services = 0\n",
    "        self.m_job_teacher = 0\n",
    "        self.f_job_at_home = 0\n",
    "        self.f_job_health = 0\n",
    "        self.f_job_other = 0\n",
    "        self.f_job_services = 0\n",
    "        self.f_job_teacher = 0\n",
    "        self.reason_course = 0\n",
    "        self.reason_home = 0\n",
    "        self.reason_other = 0\n",
    "        self.reason_reputation = 0\n",
    "        self.guardian_father = 0\n",
    "        self.guardian_mother = 0\n",
    "        self.guardian_other = 0\n",
    "        self.school_sup_no = 0\n",
    "        self.school_sup_yes = 0\n",
    "        self.fam_sup_no = 0\n",
    "        self.fam_sup_yes = 0\n",
    "        self.paid_no = 0\n",
    "        self.paid_yes = 0\n",
    "        self.activities_no = 0\n",
    "        self.activities_yes = 0\n",
    "        self.nursery_no = 0\n",
    "        self.nursery_yes = 0\n",
    "        self.higher_no = 0\n",
    "        self.higher_yes = 0\n",
    "        self.internet_no = 0\n",
    "        self.internet_yes = 0\n",
    "        self.romantic_no = 0\n",
    "        self.romantic_yes = 0\n",
    "        # Generate a default Student record\n",
    "        self.generate_record(X_actual_min, X_actual_max)\n",
    "        self.y_label = y_label\n",
    "        \n",
    "    def output_to_list(self):\n",
    "        \"\"\"\n",
    "        Outputs the internal paramters as a list to be input to a \n",
    "        Machine Learning algorithm\n",
    "        \"\"\"\n",
    "        return [self.age,\n",
    "                self.m_edu,\n",
    "                self.f_edu,\n",
    "                self.travel_time,\n",
    "                self.study_time,\n",
    "                self.failures,\n",
    "                self.fam_rel,\n",
    "                self.free_time,\n",
    "                self.go_out,\n",
    "                self.d_alc,\n",
    "                self.w_alc,\n",
    "                self.health,\n",
    "                self.absences,\n",
    "                self.g1,\n",
    "                self.g2,\n",
    "                self.school_GP,\n",
    "                self.school_MS,\n",
    "                self.sex_F,\n",
    "                self.sex_M,\n",
    "                self.address_R,\n",
    "                self.address_U,\n",
    "                self.fam_size_GT3,\n",
    "                self.fam_size_LE3,\n",
    "                self.p_status_a,\n",
    "                self.p_status_t,\n",
    "                self.m_job_at_home,\n",
    "                self.m_job_health,\n",
    "                self.m_job_other,\n",
    "                self.m_job_services,\n",
    "                self.m_job_teacher,\n",
    "                self.f_job_at_home,\n",
    "                self.f_job_health,\n",
    "                self.f_job_other,\n",
    "                self.f_job_services,\n",
    "                self.f_job_teacher,\n",
    "                self.reason_course,\n",
    "                self.reason_home,\n",
    "                self.reason_other,\n",
    "                self.reason_reputation,\n",
    "                self.guardian_father,\n",
    "                self.guardian_mother,\n",
    "                self.guardian_other,\n",
    "                self.school_sup_no,\n",
    "                self.school_sup_yes,\n",
    "                self.fam_sup_no,\n",
    "                self.fam_sup_yes,\n",
    "                self.paid_no,\n",
    "                self.paid_yes,\n",
    "                self.activities_no,\n",
    "                self.activities_yes,\n",
    "                self.nursery_no,\n",
    "                self.nursery_yes,\n",
    "                self.higher_no,\n",
    "                self.higher_yes,\n",
    "                self.internet_no,\n",
    "                self.internet_yes,\n",
    "                self.romantic_no,\n",
    "                self.romantic_yes]\n",
    "        \n",
    "    def generate_record(self, actual_min, actual_max, specific_update=-1):\n",
    "        '''\n",
    "        Randomly instantiates a sample or randomly adjusts a specific parameter.\n",
    "        Each value is normalized appropriately.\n",
    "        \n",
    "        Currently all hard-coded, should be refactored.\n",
    "        \n",
    "        Args:\n",
    "            acutal_min: A numpy array of the minimum values from the dataset\n",
    "            actual_max: A numpy array of the maximum values from the dataset\n",
    "            specific_update: If -1, randomizes all, otherwise only randomizes the specific arguement\n",
    "        '''\n",
    "        if specific_update < 0 or specific_update == 0: self.age = self.normalize_arguement(np.random.randint(15, 23), actual_min[0], actual_max[0])\n",
    "        if specific_update < 0 or specific_update == 1: self.m_edu = self.normalize_arguement(np.random.randint(0, 5), actual_min[1], actual_max[1])\n",
    "        if specific_update < 0 or specific_update == 2: self.f_edu = self.normalize_arguement(np.random.randint(0, 5), actual_min[2], actual_max[2])\n",
    "        if specific_update < 0 or specific_update == 3: self.travel_time = self.normalize_arguement(np.random.randint(1, 5), actual_min[3], actual_max[3])\n",
    "        if specific_update < 0 or specific_update == 4: self.study_time = self.normalize_arguement(np.random.randint(1, 5), actual_min[4], actual_max[4])\n",
    "        # Although failues should be [1, 4], only [1, 3] is in the dataset\n",
    "        if specific_update < 0 or specific_update == 5: self.failures = self.normalize_arguement(np.random.randint(1, 4), actual_min[5], actual_max[5])\n",
    "        if specific_update < 0 or specific_update == 6: self.fam_rel = self.normalize_arguement(np.random.randint(1, 6), actual_min[6], actual_max[6])\n",
    "        if specific_update < 0 or specific_update == 7: self.free_time = self.normalize_arguement(np.random.randint(1, 6), actual_min[7], actual_max[7])\n",
    "        if specific_update < 0 or specific_update == 7: self.go_out = self.normalize_arguement(np.random.randint(1, 6), actual_min[8], actual_max[8])\n",
    "        if specific_update < 0 or specific_update == 8: self.d_alc = self.normalize_arguement(np.random.randint(1, 6), actual_min[9], actual_max[9])\n",
    "        if specific_update < 0 or specific_update == 9: self.w_alc = self.normalize_arguement(np.random.randint(1, 6), actual_min[10], actual_max[10])\n",
    "        if specific_update < 0 or specific_update == 10: self.health = self.normalize_arguement(np.random.randint(1, 6), actual_min[11], actual_max[11])\n",
    "        if specific_update < 0 or specific_update == 11: self.absences = self.normalize_arguement(np.random.randint(0, 94), actual_min[12], actual_max[12])\n",
    "        if specific_update < 0 or specific_update == 12: self.g1 = self.normalize_arguement(np.random.randint(0, 21), actual_min[13], actual_max[13])\n",
    "        if specific_update < 0 or specific_update == 13: self.g2 = self.normalize_arguement(np.random.randint(0, 21), actual_min[14], actual_max[14])\n",
    "        if specific_update < 0 or (specific_update >= 14 and specific_update <= 15):\n",
    "            updated_values = [0, 0]\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.school_GP, self.school_MS = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 16 and specific_update <= 17):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.sex_F, self.sex_M = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 18 and specific_update <= 19):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.address_R, self.address_U = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 20 and specific_update <= 21):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.fam_size_GT3, self.fam_size_LE3 = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 22 and specific_update <= 23):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.p_status_a, self.p_status_t = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 24 and specific_update <= 28):\n",
    "            updated_values = [0] * 5\n",
    "            updated_values[np.random.randint(0, 5)] = 1\n",
    "            self.m_job_at_home, self.m_job_health, self.m_job_other, self.m_job_services, self.m_job_teacher = updated_values[0], updated_values[1], updated_values[2], updated_values[3], updated_values[4]\n",
    "        if specific_update < 0 or (specific_update >= 29 and specific_update <= 33):\n",
    "            updated_values = [0] * 5\n",
    "            updated_values[np.random.randint(0, 5)] = 1\n",
    "            self.f_job_at_home, self.f_job_health, self.f_job_other, self.f_job_services, self.f_job_teacher = updated_values[0], updated_values[1], updated_values[2], updated_values[3], updated_values[4]\n",
    "        if specific_update < 0 or (specific_update >= 34 and specific_update <= 37):\n",
    "            updated_values = [0] * 4\n",
    "            updated_values[np.random.randint(0, 4)] = 1\n",
    "            self.reason_course, self.reason_home, self.reason_other, self.reason_reputation = updated_values[0], updated_values[1], updated_values[2], updated_values[3]\n",
    "        if specific_update < 0 or (specific_update >= 38 and specific_update <= 40):\n",
    "            updated_values = [0] * 3\n",
    "            updated_values[np.random.randint(0, 3)] = 1\n",
    "            self.guardian_father, self.guardian_mother, self.guardian_other = updated_values[0], updated_values[1], updated_values[2]\n",
    "        if specific_update < 0 or (specific_update >= 41 and specific_update <= 42):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.school_sup_no, self.school_sup_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 43 and specific_update <= 44):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.fam_sup_no, self.fam_sup_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 45 and specific_update <= 46):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.paid_no, self.paid_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 47 and specific_update <= 48):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.activities_no, self.activities_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 49 and specific_update <= 50):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.nursery_no, self.nursery_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 51 and specific_update <= 52):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.higher_no, self.higher_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 53 and specific_update <= 54):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.internet_no, self.internet_yes = updated_values[0], updated_values[1]\n",
    "        if specific_update < 0 or (specific_update >= 55 and specific_update <= 56):\n",
    "            updated_values = [0] * 2\n",
    "            updated_values[np.random.randint(0, 2)] = 1\n",
    "            self.romantic_no, self.romantic_yes = updated_values[0], updated_values[1]\n",
    "            \n",
    "    def normalize_arguement(self, value, actual_min, actual_max):\n",
    "        \"\"\"\n",
    "        Normalizes an individual parameter.\n",
    "        \"\"\"\n",
    "        return (value - actual_min) / (actual_max - actual_min)\n",
    "        \n",
    "    def randomize_k_features(self, k):\n",
    "        \"\"\"\n",
    "        Out of the 58 features used, randomly update k of the features.\n",
    "        \"\"\"\n",
    "        TOTAL_ELEMENTS = 58\n",
    "        random_choice = np.random.choice(TOTAL_ELEMENTS, TOTAL_ELEMENTS, replace=False)\n",
    "        for index in random_choice[:k]:\n",
    "            self.generate_record(X_actual_min, X_actual_max, index)\n",
    "        return random_choice[:k]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Determines whether two student record are equivalent.\n",
    "        \"\"\"\n",
    "        return self.__dict__ == other.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate and view a synthetic student sample\n",
    "temp = StudentRecord(1, X_actual_min, X_actual_max)\n",
    "print(temp.output_to_list())\n",
    "temp.randomize_k_features(5)\n",
    "print(temp.output_to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the class of the synthetic sample from the host model (just to view how it works)\n",
    "np.argmax(model.predict([temp.output_to_list()]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How long to proceed until stopping\n",
    "MAX_ITERATIONS = 100000\n",
    "# Maximum number of potential rejections before restarting\n",
    "MAX_REJECTIONS = 50\n",
    "# Minimum class confidence percentage needed from the target model when\n",
    "# predicting the class of the synthetic sample\n",
    "# The higher the value, the more similar the datapoints become\n",
    "MIN_CONFIDENCE = 0.65\n",
    "def synthesize(y_label, X_actual_min, X_actual_max, model):\n",
    "    '''\n",
    "    Generates synthetic datapoints which are are considered to be close enough to\n",
    "    the target class when predicted by the target model.\n",
    "    \n",
    "    Args:\n",
    "        y_label: The class label to be assigned to the sample (0 or 1)\n",
    "        X_acutal_min: A numpy array of the minimum values from the dataset\n",
    "        X_actual_max: A numpy array of the maximum values from the dataset\n",
    "    index of prediction must equal class label\n",
    "    '''\n",
    "    # Generate the initial randomized student record\n",
    "    student = StudentRecord(y_label, X_actual_min, X_actual_max)\n",
    "    # Track the confidence of the record generated\n",
    "    received_class_conf = 0\n",
    "    # Track the numer of rejections\n",
    "    j = 0\n",
    "    # How many parameters to randomize at a time\n",
    "    k = 14\n",
    "    for i in range(MAX_ITERATIONS):\n",
    "        # Query the target model\n",
    "        y_query = model.predict([student.output_to_list()], verbose=0)\n",
    "        #print(y_query)\n",
    "        # Continue if the predicted confidence is higher than the last attempt\n",
    "        if y_query[0][y_label] > received_class_conf:\n",
    "            # Get the index from the query output vector that has max prob.\n",
    "            c =  np.argmax(y_query, axis=1)[0]\n",
    "            # Verify that the confidence is higher than the threshold, \n",
    "            # that the predicted label is the target label, and that the\n",
    "            # confidence passes a random threshold\n",
    "            if y_query[0][y_label] > MIN_CONFIDENCE and c == y_label and np.random.random() < y_query[0][y_label]:\n",
    "                # Return the synthetic sample\n",
    "                return student\n",
    "            # Update the confidence if not accepted, but was higher than the\n",
    "            # previous value\n",
    "            received_class_conf = y_query[0][y_label]\n",
    "            # Reset the number of rejections\n",
    "            j = 0\n",
    "        else:\n",
    "            # Update the number of rejections and adjust k\n",
    "            j += 1\n",
    "            if j > MAX_REJECTIONS:\n",
    "                k = max(1, math.ceil(k / 2))\n",
    "                j = 0\n",
    "        # Randomize k features of the synthetic sample\n",
    "        student.randomize_k_features(k)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a random synthetic sample for class=0\n",
    "s = synthesize(0, X_actual_min, X_actual_max, model)\n",
    "print(s.output_to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TESTING CODE BLOCK (IGNORE)\n",
    "a = []\n",
    "b = [[1, 1],[2,2] ,[3,3] ]\n",
    "c  =[[4,4], [5,5], [6,6]]\n",
    "a.append(np.append(b, c, axis=0))\n",
    "np.random.shuffle(a[0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create k datasets for k shadow models\n",
    "# The number of samples chosen will result in double that total number of \n",
    "# samples being used when combined as a train/test set\n",
    "NUM_SAMPLES_PER_CLASS = 250\n",
    "NUM_SHADOW_MODELS = 1\n",
    "# Ensure reproducability\n",
    "np.random.seed(1)\n",
    "# Track the generated datasets\n",
    "k_datasets = []\n",
    "print(\"Starting to generate synthetic datasets...\")\n",
    "# Multiply by two since one will be for training and one will be for testing\n",
    "# - i % 2 == 0 -> training set\n",
    "# - i % 2 == 1 -> testing set\n",
    "for i in range(NUM_SHADOW_MODELS * 2):\n",
    "    print(\"CREATING DATASET:\")\n",
    "    # Generate the synthetic samples for class=0 and class=1\n",
    "    students_0 = [synthesize(0, X_actual_min, X_actual_max, model) for _ in range(NUM_SAMPLES_PER_CLASS)]\n",
    "    students_1 = [synthesize(1, X_actual_min, X_actual_max, model) for _ in range(NUM_SAMPLES_PER_CLASS)]\n",
    "    k_datasets.append(np.append(students_0, students_1, axis=0))\n",
    "    # Ensure that the test dataset does not contain datapoints from the train dataset\n",
    "    if i % 2 == 1:\n",
    "        for j in range(len(k_datasets[i])):\n",
    "            unique_entry = False\n",
    "            while not unique_entry:\n",
    "                unique_entry = True\n",
    "                # Determine whether any duplicates are found\n",
    "                for student in k_datasets[i - 1]:\n",
    "                    if k_datasets[i][j] == student:\n",
    "                        print(\"Redoing entry\", j)\n",
    "                        unique_entry = False\n",
    "                        k_datasets[i][j] = synthesize(1, X_actual_min, X_actual_max, model)\n",
    "                        break\n",
    "    # Shuffle the generated data\n",
    "    np.random.shuffle(k_datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train each of the k shadow models with the same model setup as the normal model\n",
    "models = []\n",
    "for i in range(0, len(k_datasets), 2):\n",
    "    # TODO: ADD test sets into function input and change to be x,y\n",
    "    trained_model = train(\n",
    "        noise_multiplier=1,\n",
    "        l2_norm_clip=1,\n",
    "        batch_size=22,\n",
    "        microbatches=11,\n",
    "        x_train=[elem.output_to_list() for elem in k_datasets[i]],\n",
    "        y_train=[elem.y_label for elem in k_datasets[i]],\n",
    "        dpsgd=False,\n",
    "        learning_rate=0.01,\n",
    "        epochs=1000,\n",
    "        model_dir=None,\n",
    "        print_outputs=True,\n",
    "    )[-1]\n",
    "    models.append(trained_model)\n",
    "\n",
    "\n",
    "# Then we can synthesize data likely in the dataset and extract it\n",
    "...\n",
    "# Formulate evaluation metrics\n",
    "...\n",
    "# Test on a DP model\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each sample in the train and test sets for a model i, get the predictions \n",
    "# from that model and add it to the sample as a new feature\n",
    "k_prediction_sets = []\n",
    "for i in range(0, len(k_datasets), 2):\n",
    "    print(\"Generating probabilities...\")\n",
    "    predictions_train = models[i // 2].predict([elem.output_to_list() for elem in k_datasets[i]])\n",
    "    predictions_test = models[i // 2].predict([elem.output_to_list() for elem in k_datasets[i + 1]])\n",
    "    k_prediction_sets.append(predictions_train)\n",
    "    k_prediction_sets.append(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an attack model for both class=0 and class=1 and train to predict 'no' or 'yes' on whether the data was used to train the shadow models\n",
    "# Requires: initial_class_label, output_vector, in_or_out\n",
    "# Initialize an attack model for each initial_class_label which uses the output_vector to predict in_or_out\n",
    "# Define a sequential Keras model\n",
    "def generate_attack_model(learning_rate=0.001):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(64, input_dim=2, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(rate=0.1, noise_shape=None, seed=1), # 0.1 was good\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "            #tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            # CHANGED: V2 for mia attack we need logit output\n",
    "            tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    # Compile model with Keras\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the learning rate to use\n",
    "LR = 0.1\n",
    "attack_models = [generate_attack_model(LR), generate_attack_model(LR)]\n",
    "# Track the samples and labels for the attack model for a specific class\n",
    "X_attacks = [[], []]\n",
    "y_attacks = [[], []]\n",
    "for i in range(len(k_prediction_sets)):\n",
    "    # Add all probabilities for samples which have a class of 0\n",
    "    attack_train_dataset_0 = [k_prediction_sets[i][j] for j in range(len(k_prediction_sets[i])) if k_datasets[i][j].y_label == 0]\n",
    "    attack_train_dataset_1 = [k_prediction_sets[i][j] for j in range(len(k_prediction_sets[i])) if k_datasets[i][j].y_label == 1]\n",
    "    X_attacks[0] += attack_train_dataset_0\n",
    "    X_attacks[1] += attack_train_dataset_1\n",
    "    # Below ensures that 1 is in the training set and 0 is outside the training set\n",
    "    y_attacks[0] += [((i + 1) % 2)] * len(attack_train_dataset_0)\n",
    "    y_attacks[1] += [((i + 1) % 2)] * len(attack_train_dataset_1)\n",
    "# Convert the lists to nupy arrays\n",
    "X_attacks[0], X_attacks[1] = np.array(X_attacks[0]), np.array(X_attacks[1])\n",
    "y_attacks[0], y_attacks[1] = np.array(y_attacks[0]), np.array(y_attacks[0])\n",
    "print(attack_models[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure that the number of labels and samples (the probabilities) match\n",
    "assert(len(y_attacks[0]) == len(X_attacks[0]))\n",
    "assert(len(y_attacks[1]) == len(X_attacks[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_histories = []\n",
    "# Train model with Keras for class=0\n",
    "for i in range(len(attack_models)):\n",
    "    history = attack_models[i].fit(\n",
    "        X_attacks[i],\n",
    "        y_attacks[i],\n",
    "        epochs=2000,\n",
    "        #shuffle=True,\n",
    "        batch_size=64\n",
    "    )\n",
    "    train_histories.append(history)\n",
    "#64: 0.58 max\n",
    "#16: 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have the attack models and the model\n",
    "# Run the synthetic data through the model, get the probabilities, then run through the attack models\n",
    "print(\"Getting statistics for class = 0\")\n",
    "y_pred_0 = attack_models[0].predict(x=X_attacks[0])\n",
    "print(classification_report(y_attacks[0], np.argmax(y_pred_0, axis=1)))\n",
    "print(\"\\nGetting statistics for class = 1\")\n",
    "y_pred_1 = attack_models[1].predict(x=X_attacks[1])\n",
    "print(classification_report(y_attacks[1], np.argmax(y_pred_1, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the probabilities for the train/test sets\n",
    "actual_prediction_sets = []\n",
    "for i in range(0, len(k_datasets), 2):\n",
    "    print(\"Generating probabilities...\")\n",
    "    predictions_train = models[i // 2].predict(x_train)\n",
    "    predictions_test = models[i // 2].predict(x_test)\n",
    "    actual_prediction_sets.append(predictions_train)\n",
    "    actual_prediction_sets.append(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the input data of the actual dataset to input to the attack model\n",
    "actual_X_attacks = [[], []]\n",
    "actual_y_attacks = [[], []]\n",
    "y_combined = [y_train, y_test]\n",
    "for i in range(len(actual_prediction_sets)):\n",
    "    # Add all probabilities for samples which have a class of 0\n",
    "    attack_train_dataset_0 = [actual_prediction_sets[i][j] for j in range(len(actual_prediction_sets[i])) if y_combined[i][j] == 0]\n",
    "    attack_train_dataset_1 = [actual_prediction_sets[i][j] for j in range(len(actual_prediction_sets[i])) if y_combined[i][j] == 1]\n",
    "    actual_X_attacks[0] += attack_train_dataset_0\n",
    "    actual_X_attacks[1] += attack_train_dataset_1\n",
    "    # Below ensures that 1 is in the training set and 0 is outside the training set\n",
    "    actual_y_attacks[0] += [((i + 1) % 2)] * len(attack_train_dataset_0)\n",
    "    actual_y_attacks[1] += [((i + 1) % 2)] * len(attack_train_dataset_1)\n",
    "actual_X_attacks[0], actual_X_attacks[1] = np.array(actual_X_attacks[0]), np.array(actual_X_attacks[1])\n",
    "actual_y_attacks[0], actual_y_attacks[1] = np.array(actual_y_attacks[0]), np.array(actual_y_attacks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have the attack models and the model\n",
    "# Run the original data through the model, get the probabilities, then run through the attack models\n",
    "print(\"Testing accuracy for class = 0\")\n",
    "attack_models[0].evaluate(x=actual_X_attacks[0], y=actual_y_attacks[0])\n",
    "print(\"Testing accuracy for class = 1\")\n",
    "attack_models[1].evaluate(x=actual_X_attacks[1], y=actual_y_attacks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have the attack models and the model\n",
    "# Run the original data through the model, get the probabilities, then run through the attack models\n",
    "print(\"Getting statistics for class = 0\")\n",
    "y_pred_0 = attack_models[0].predict(x=actual_X_attacks[0])\n",
    "print(classification_report(actual_y_attacks[0], np.argmax(y_pred_0, axis=1)))\n",
    "print(\"\\nGetting statistics for class = 1\")\n",
    "y_pred_1 = attack_models[1].predict(x=actual_X_attacks[1])\n",
    "print(classification_report(actual_y_attacks[1], np.argmax(y_pred_1, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
