{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCtZONh3yU4"
      },
      "source": [
        "#**Secure Federated Weighted Average strategy for imbalanced datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrYSzFamz37m"
      },
      "source": [
        "#Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q69zTMh2dRH"
      },
      "source": [
        "*   Pytorch and flower installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GwEkQ0Jz37n"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision opacus gmpy2 pympler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download and install Paillier wrapper library"
      ],
      "metadata": {
        "id": "oib8KhBA-XH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1sU2Z1S1jbpA-GS2Rc9PJU5i--nzj2aJs\n",
        "!pip install -q simplephe-0.0.1-py3-none-any.whl"
      ],
      "metadata": {
        "id": "NaAQZ0jv992l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qls8qd8Iz37p"
      },
      "source": [
        "##All General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-1yvWzz37p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "import timeit\n",
        "import platform\n",
        "\n",
        "from functools import reduce\n",
        "from collections import OrderedDict\n",
        "from hashlib import md5\n",
        "from pympler import asizeof\n",
        "from copy import deepcopy\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union, NewType\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.accountants.rdp import RDPAccountant\n",
        "\n",
        "from scipy.stats import entropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seaborn plot settings\n",
        "sns.set_style(\"white\")\n",
        "#palette = sns.color_palette(\"Set2\")\n",
        "palette = ['red','blue','green','grey','brown','violet','cyan']\n",
        "sns.set_context(\"paper\", font_scale=1.2)  # Increase font size"
      ],
      "metadata": {
        "id": "1GyzoTt4np3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216WwsYEXg2j"
      },
      "source": [
        "##All Machine Learning Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMhXiHTAXhbQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch import Tensor\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from collections import Counter, OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o51lANbXqun"
      },
      "source": [
        "##All Federated Learning Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sB9T5rcXjK8"
      },
      "outputs": [],
      "source": [
        "import flwr as fl\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-u7W7RkYcVL"
      },
      "outputs": [],
      "source": [
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitRes,\n",
        "    Parameters,\n",
        "    Scalar,\n",
        "    NDArrays,\n",
        "    parameters_to_ndarrays,\n",
        "    ndarrays_to_parameters,\n",
        "    MetricsAggregationFn\n",
        ")\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.strategy import fedavg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Tested with flower version 1.5.0 and torch version 2.0.1+cu118**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4dmbcwvkdnl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6qMm3gfaIFL"
      },
      "outputs": [],
      "source": [
        "fl.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I44eoVAwaXNC"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homomorphic Encryption"
      ],
      "metadata": {
        "id": "PiL-Yl5k_kSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import simplephe as sp"
      ],
      "metadata": {
        "id": "YxlPGb5y-1TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIvurwkgP3d3"
      },
      "source": [
        "##Reproducibility Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1KM00k3P2CW"
      },
      "outputs": [],
      "source": [
        "# For dataloader workers\n",
        "def _init_fn(worker_id):\n",
        "    np.random.seed(int(random_seed))\n",
        "\n",
        "\n",
        "def set_random_seeds(random_seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "random_seed = 123\n",
        "set_random_seeds(random_seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQQ--21w4ImG"
      },
      "source": [
        "##All Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQRPcxbv4HEh"
      },
      "outputs": [],
      "source": [
        "experiment_params = {}\n",
        "strategy_colors = ['red', 'green', 'blue', 'cyan']\n",
        "#strategy_list = ['fedavg', 'fedadagrad', 'weightedfedavg', 'fedadam', 'fedyogi']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqbmPw1SEj47"
      },
      "outputs": [],
      "source": [
        "# @title Globals { display-mode: \"form\" }\n",
        "# @markdown Number of federated clients:\n",
        "n_clients = 8 # @param {type:\"slider\", min:6, max:10, step:1}\n",
        "loop_on_strategies = True # @param {type:\"boolean\"}\n",
        "USE_DP = False # @param {type:\"boolean\"}\n",
        "target_epsilon = 0.3 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "experiment_params[\"n_clients\"] = n_clients\n",
        "experiment_params[\"loop_on_strategies\"] = loop_on_strategies\n",
        "experiment_params[\"USE_DP\"] = USE_DP\n",
        "if USE_DP:\n",
        "  experiment_params[\"target_epsilon\"] = target_epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfTfnZE2V4cq"
      },
      "outputs": [],
      "source": [
        "# @title Default strategy { display-mode: \"form\" }\n",
        "strategy_type = 'secwfedavg' # @param ['fedavg', 'fedadagrad', 'weightedfedavg', 'secwfedavg','fedadam', 'fedyogi']\n",
        "# @markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title #### Strategies\n",
        "strategy_list = []\n",
        "if loop_on_strategies:\n",
        "  strategy_list = ['fedavg', 'weightedfedavg', 'secwfedavg']\n",
        "  experiment_params[\"strategy_list\"] = strategy_list\n",
        "  print(strategy_list)\n",
        "else:\n",
        "  experiment_params[\"strategy_type\"] = strategy_type\n",
        "  print(strategy_type)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B7iudw49Zto1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4cXeVEF91j7"
      },
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2BhNjhC94dt"
      },
      "outputs": [],
      "source": [
        "# @title ### Hypers { display-mode: \"form\" }\n",
        "shallow_model = True # @param {type:\"boolean\"}\n",
        "n_epochs = 2 # @param {type:\"slider\", min:1, max:25}\n",
        "n_rounds = 15 # @param {type:\"slider\", min:2, max:25}\n",
        "batch_size = 32 # @param {type:\"slider\", min:32, max:128, step:32}\n",
        "validation_split = 0.2\n",
        "learning_rate = 0.003190727031874879 # @param {type:\"number\"}\n",
        "# for shallow model 0.003190727031874879 else 0.0018673528886359607\n",
        "# @markdown ---\n",
        "experiment_params[\"shallow_model\"] = n_epochs\n",
        "experiment_params[\"n_epochs\"] = n_epochs\n",
        "experiment_params[\"n_rounds\"] = n_rounds\n",
        "experiment_params[\"batch_size\"] = batch_size\n",
        "experiment_params[\"learning_rate\"] = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Split method { display-mode: \"form\" }\n",
        "# @markdown Select method:\n",
        "# @markdown - majority = one majority class per client. Different sample size per client.\n",
        "# @markdown - majority_even = one majority class per client. Same sample size per client.\n",
        "# @markdown - pick_two = two majority class per client\n",
        "# @markdown - random = random splits\n",
        "method_selected = 'majority' # @param ['stratified', 'random', 'majority_even', 'majority', 'pick_two']\n",
        "# @markdown Ratio for majority classes\n",
        "# *Not all values are possible*\n",
        "# @markdown **Bug:** *Not all values are possible*, it gives an error if there are not enough samples to distribute to all clients.\n",
        "ratio_majority_class = 0.5 # @param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "test_split_size = 0.25 # @param {type:\"slider\", min:0.1, max:0.5, step:0.05}\n",
        "# @markdown ---\n",
        "experiment_params[\"method_selected\"] = method_selected\n",
        "experiment_params[\"ratio_majority_class\"] = ratio_majority_class\n",
        "experiment_params[\"test_split_size\"] = test_split_size"
      ],
      "metadata": {
        "id": "4RMI1impWqO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Weighting options { display-mode: \"form\" }\n",
        "# @markdown Options:\n",
        "# @markdown - Weighted -> using sample sizes and class frequencies\n",
        "# @markdown - Standard = FedAvg\n",
        "# @markdown - Arithmetic = Naive average (not weighted)\n",
        "avg_strategy = 'weighted' # @param [\"standard\", \"arithmetic\", \"weighted\"]\n",
        "# @markdown ---\n",
        "experiment_params[\"avg_strategy\"] = avg_strategy"
      ],
      "metadata": {
        "id": "FhSsQH28XAu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p9VHdJ-2-gu"
      },
      "source": [
        "##Initializations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Encrypted layers selection\n",
        "# @markdown Specify layer indices to encrypt:\n",
        "# @markdown - all weights [0, 2, 4, 5]\n",
        "# @markdown - all weights and non linear layers [i for i in range(6)]\n",
        "# @markdown - some layers [2,4,5]\n",
        "# @markdown - for testing use layer [5]\n",
        "\n",
        "# @markdown For the shallow model:\n",
        "# @markdown - all weights [0, 2, 3]\n",
        "# @markdown - all weights and non linear layers [i for i in range(4)]\n",
        "# @markdown - some layers [2, 3]\n",
        "# @markdown - for testing use layer [3]\n",
        "encrypted_layers_ids = [0, 2, 3]\n",
        "experiment_params[\"encrypted_layers_ids\"] = encrypted_layers_ids"
      ],
      "metadata": {
        "id": "XYr0tPVrya6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_json = json.dumps(experiment_params)"
      ],
      "metadata": {
        "id": "CTgO_BINiU-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save path\n",
        "save_path = md5(experiment_json.encode()).hexdigest()[:8]\n",
        "print(save_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UHsmZIx8i7Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Prefix for experiment folder\n",
        "prefix = \"shallow-layers-0-2-3-rounds-15-epochs-2\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4iFxC9sgJ_8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = f\"{prefix}_{save_path}\"\n",
        "with open(f'{save_path}.json', 'w') as f:\n",
        "    f.write(experiment_json)"
      ],
      "metadata": {
        "id": "eCXyPcfHiBGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDtG-ecZ2_Rl"
      },
      "outputs": [],
      "source": [
        "start_global_time = timeit.default_timer()\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "with open(f'{save_path}/experiment_parameters.json', 'w') as f:\n",
        "    f.write(experiment_json)\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Prova \"cuda\" per addestramento su GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")\n",
        "\n",
        "OS = platform.system()           # Sistema Operativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3-9cMBkbAsq"
      },
      "source": [
        "#Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmAPufD2Gz8-"
      },
      "source": [
        "##Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9EGcGI8bNeq"
      },
      "outputs": [],
      "source": [
        "def data_download(file_to_download, gdrive_code, OS, uncompress = True):\n",
        "  if not os.path.exists(file_to_download):\n",
        "    os.system('gdown --id \"'+gdrive_code+'\" --output '+file_to_download)\n",
        "    if OS == \"Linux\" and uncompress:\n",
        "        os.system('unzip -o -n \"./'+file_to_download+'\" -d '+os.path.dirname(file_to_download))\n",
        "    return True\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj3J-tRF6ebG"
      },
      "outputs": [],
      "source": [
        "out = data_download(\"./har_datasets_fl.zip\", \"1LUjU4yvBRh6FPBlIHRCD2uf5zMH6l9tC\", OS)\n",
        "#urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\", filename=\"har-data.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGxcJLB06v-9"
      },
      "source": [
        "##Data Splitting\n",
        "\n",
        "Proposed methods:\n",
        "- Majority even: a majority class will be distributed to each client $i=\\{1\\to \\text{num classes}\\}$ with a custom ratio ensuring that the splits for all clients have the same number of samples.\n",
        "- Majority: as above but splits do not have the same size.\n",
        "- Pick two: as the first method but distributing two \"majority\" classes.\n",
        "- Random: distributes samples randomly to each client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CPwBtij5TKD"
      },
      "outputs": [],
      "source": [
        "n_splits = n_clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3nf65Y1Fzg9"
      },
      "outputs": [],
      "source": [
        "trainloaders = []\n",
        "\n",
        "# Awful hack, when True this flips test and train datasets for a stratified\n",
        "# split ensuring that independent balanced samples are distributed in each split\n",
        "# Normal behavior flip=False\n",
        "flip = False\n",
        "def stratified_split(data, targets, n_splits, split_size=None):\n",
        "    # NOTE: We pick one stratified split => n_splits=1 because we want a\n",
        "    # balanced test set, the training part will be postprocessed\n",
        "    if not split_size:\n",
        "      df = pd.DataFrame(data)\n",
        "      data_length = len(df)\n",
        "      split_size = int(data_length / n_splits)\n",
        "      print(\"split_size\", test_size)\n",
        "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=split_size, random_state=random_seed)\n",
        "    for train_index, val_index in sss.split(data, targets):\n",
        "        if flip:\n",
        "          yield data[val_index], targets[val_index], data[train_index], targets[train_index]\n",
        "        else:\n",
        "          yield data[train_index], targets[train_index], data[val_index], targets[val_index]\n",
        "\n",
        "def random_split(data, targets, n_splits, split_size=None):\n",
        "    if not split_size:\n",
        "      split_size = 1 / n_splits\n",
        "    for _ in range(n_splits):\n",
        "        X_train, X_val, y_train, y_val = train_test_split(data, targets, test_size=split_size)\n",
        "        yield X_train, y_train, X_val, y_val\n",
        "\n",
        "def majority_even(data, targets, n_splits, split_size=None,\n",
        "                        ratio_majority_class=ratio_majority_class):\n",
        "    df = pd.DataFrame(data)\n",
        "    df['Y'] = targets\n",
        "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "    datasets = []\n",
        "    data_length = len(df)\n",
        "    split_size = int(data_length / n_splits)\n",
        "    class_counts = df['Y'].value_counts().to_dict()\n",
        "    for cls, count in class_counts.items():\n",
        "        majority_sample = df[df['Y'] == cls].sample(int(count * ratio_majority_class))\n",
        "        #add second majority class for 3rd first datasets\n",
        "        other_classes_sample = df[~(df['Y'] == cls)].sample(split_size - len(majority_sample))\n",
        "        dataset = pd.concat([majority_sample, other_classes_sample]).sample(frac=1).reset_index(drop=True)\n",
        "        datasets.append(dataset)\n",
        "        used_indices = pd.Index(majority_sample.index).union(other_classes_sample.index)\n",
        "        df.drop(used_indices, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for _ in range(n_splits -len(datasets)):\n",
        "        dataset = df.sample(split_size)\n",
        "        datasets.append(dataset)\n",
        "        df.drop(dataset.index, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for dataset in datasets:\n",
        "        X_train = dataset.drop(columns=['Y']).to_numpy()\n",
        "        y_train = dataset['Y'].to_numpy()\n",
        "        yield X_train, y_train, None, None\n",
        "\n",
        "def pick_two(data, targets, n_splits, split_size=None,\n",
        "                        ratio_majority_class=ratio_majority_class):\n",
        "    df = pd.DataFrame(data)\n",
        "    df['Y'] = targets\n",
        "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "    datasets = []\n",
        "    data_length = len(df)\n",
        "    split_size = int(data_length / n_splits)\n",
        "    class_counts = df['Y'].value_counts().to_dict()\n",
        "    for (cls1, count1), (cls2, count2)  in zip(list(class_counts.items())[::2],\n",
        "                                               list(class_counts.items())[1::2]):\n",
        "        print((cls1, count1), (cls2, count2))\n",
        "        majority_sample = df[(df['Y'] == cls1) | (df['Y']==cls2)].sample(int(count1 * ratio_majority_class))\n",
        "        other_classes_sample = df[~((df['Y'] == cls1) | (df['Y']==cls2))].sample(split_size - len(majority_sample))\n",
        "        dataset = pd.concat([majority_sample, other_classes_sample]).sample(frac=1).reset_index(drop=True)\n",
        "        datasets.append(dataset)\n",
        "        used_indices = pd.Index(majority_sample.index).union(other_classes_sample.index)\n",
        "        df.drop(used_indices, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for _ in range(n_splits -len(datasets)):\n",
        "        dataset = df.sample(split_size)\n",
        "        datasets.append(dataset)\n",
        "        df.drop(dataset.index, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for dataset in datasets:\n",
        "        X_train = dataset.drop(columns=['Y']).to_numpy()\n",
        "        y_train = dataset['Y'].to_numpy()\n",
        "        yield X_train, y_train, None, None\n",
        "\n",
        "def pick_majority(data, targets, n_splits, split_size=None,\n",
        "                          percent_majority_class=ratio_majority_class):\n",
        "    df = pd.DataFrame(data)\n",
        "    df['Y'] = targets\n",
        "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "    maj_datasets = []\n",
        "    data_length = len(df)\n",
        "    class_counts = df['Y'].value_counts().to_dict()\n",
        "    for cls, count in class_counts.items():\n",
        "        majority_sample = df[df['Y'] == cls].sample(int(count * ratio_majority_class))\n",
        "        maj_datasets.append(majority_sample)\n",
        "        used_indices = pd.Index(majority_sample.index)\n",
        "        df.drop(used_indices, inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    splits = np.array_split(df, n_splits)\n",
        "    datasets = []\n",
        "    for i, split in enumerate(splits):\n",
        "        if i < len(np.unique(targets)):\n",
        "          dataset = pd.concat([pd.DataFrame(split), maj_datasets[i]]).sample(frac=1).reset_index(drop=True)\n",
        "        else:\n",
        "          dataset = pd.DataFrame(split)\n",
        "        datasets.append(dataset)\n",
        "\n",
        "    for dataset in datasets:\n",
        "        X_train = dataset.drop(columns=['Y']).to_numpy()\n",
        "        y_train = dataset['Y'].to_numpy()\n",
        "        yield X_train, y_train, None, None\n",
        "\n",
        "\n",
        "SPLIT_METHODS = {\n",
        "    'stratified': stratified_split,\n",
        "    'random': random_split,\n",
        "    'majority_even': majority_even,\n",
        "    'majority': pick_majority,\n",
        "    'pick_two': pick_two\n",
        "}\n",
        "\n",
        "def gini_index(y):\n",
        "  uniques = np.unique(y+1, return_counts=True)\n",
        "  probs = uniques[1]/np.sum(uniques[1])\n",
        "  #print(uniques, probs, np.sum(probs))\n",
        "  gini_index = 1.0 - np.sum(probs ** 2)\n",
        "  return gini_index\n",
        "\n",
        "def get_data_from_path(path):\n",
        "    fold_number = os.path.basename(path).split('-')[0].strip()\n",
        "    trainset = pd.read_csv(f\"{path}/train/{fold_number}_ALL_train.csv\", delimiter=';')\n",
        "    testset = pd.read_csv(f\"{path}/test/{fold_number}_ALL_test.csv\", delimiter=';')\n",
        "    return trainset, testset\n",
        "\n",
        "def create_datasets_from_dataframe(df):\n",
        "    # Extract features from columns '0' to '560'\n",
        "    X = pd.concat([df[str(i)] for i in range(561)], axis=1).values\n",
        "    # Adjust labels in 'Y' column to start from 0\n",
        "    y = (df['Y'] - 1).values\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def generate_dataloaders(data, targets, split_method, n_splits):\n",
        "    dataloaders = []\n",
        "    split_function = SPLIT_METHODS[split_method]\n",
        "    gini_indices = []\n",
        "    # Assuming set_random_seeds function is defined elsewhere\n",
        "    set_random_seeds(random_seed)\n",
        "\n",
        "    for i, (X_train, y_train, _, _) in\\\n",
        "     enumerate(split_function(data, targets, n_splits)):\n",
        "        train_gini = gini_index(y_train)\n",
        "\n",
        "\n",
        "        # Print distribution of classes for this fold\n",
        "        class_distribution = Counter(y_train)\n",
        "        print(f\"Fold {i+1} size {len(y_train)} class distribution: {class_distribution}\")\n",
        "\n",
        "        gini_data = {\n",
        "            'Dataset': i + 1,\n",
        "            'Train Gini Index': train_gini,\n",
        "        }\n",
        "\n",
        "        gini_indices.append(gini_data)\n",
        "\n",
        "        train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).long())\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        dataloaders.append(train_loader)\n",
        "\n",
        "    return dataloaders, gini_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSoasfzhgUT"
      },
      "outputs": [],
      "source": [
        "# Let's combine the old data splits into a single dataframe\n",
        "all_data = []\n",
        "for path in [f.path for f in os.scandir('./har_datasets_fl') if f.is_dir()]:\n",
        "    train_df, test_df = get_data_from_path(path)\n",
        "    all_data.append(train_df)\n",
        "    all_data.append(test_df)\n",
        "\n",
        "combined_df = pd.concat(all_data, axis=0)\n",
        "print(f\"Total data points {len(combined_df)}\")\n",
        "\n",
        "X_all, y_all = create_datasets_from_dataframe(combined_df)\n",
        "\n",
        "# 1st stratified to get all train data and test data for (server) evaluation\n",
        "test_size = 0.2\n",
        "X_train_combined, y_train_combined, X_test, y_test =\\\n",
        "  next(stratified_split(X_all, y_all, n_splits=1, split_size=test_split_size))\n",
        "\n",
        "print(f\"Total train data points {len(X_train_combined)}\")\n",
        "print(f\"Total train data points {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhCBySEXS_cv"
      },
      "source": [
        "#### Baseline\n",
        "The datasets for the baseline contains independent stratified balanced folds, one per client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTcyq1OxLNBz"
      },
      "outputs": [],
      "source": [
        "flip = True\n",
        "trainloaders_bl, gini_data_bl = \\\n",
        "  generate_dataloaders(X_train_combined, y_train_combined,\n",
        "                       \"stratified\", n_splits)\n",
        "\n",
        "gini_df = pd.DataFrame(gini_data_bl)\n",
        "print(gini_df)\n",
        "\n",
        "print(\"Number of Training Subsets: \", len(trainloaders_bl))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes\n",
        "n_classes = len(np.unique(y_train_combined))\n",
        "# Assuming class_names is a dictionary mapping class numbers to class names\n",
        "class_names = {0: \"Walking\", 1: \"Walking\\nupstairs\", 2: \"Walking\\ndownstairs\", 3: \"Sitting\", 4: \"Standing\", 5: \"Laying\"}"
      ],
      "metadata": {
        "id": "3REVpi7lOiFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUG8sALzTd_z"
      },
      "outputs": [],
      "source": [
        "def plot_classes(trainloaders):\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "  ax1.set_title(\"Classes distributions\")\n",
        "  offset=-0.4\n",
        "  counts_per_client = [pd.DataFrame(np.concatenate(tuple([y.numpy() for x, y  in t])), columns=[\"class\"]).value_counts() for t in trainloaders]\n",
        "\n",
        "  df = pd.concat([pd.DataFrame(ser).assign(client=i) for i, ser in enumerate(counts_per_client)]).reset_index().sort_values(by=[\"client\", \"class\"]).replace({'class': class_names})\n",
        "\n",
        "  ax2.set_title(\"Client distributions\")\n",
        "  sns.barplot(x=\"class\", y=0, hue=\"client\", data=df, ax=ax1)\n",
        "  ax1.set_ylabel(\"Counts\")\n",
        "  ax1.set_xlabel(\"Classes\")\n",
        "  sns.barplot(x=\"client\", y=0, hue=\"class\", data=df, ax=ax2)\n",
        "  ax2.set_xlabel(\"Client #\")\n",
        "  ax2.set_ylabel(\"Counts\")\n",
        "  plt.show()\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KKrxR2NTfJK"
      },
      "outputs": [],
      "source": [
        "df_bl = plot_classes(trainloaders_bl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TliL28UyTV_A"
      },
      "source": [
        "#### Dataloaders for split datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGH8rINRYfPK"
      },
      "outputs": [],
      "source": [
        "# get dataloaders\n",
        "flip = False\n",
        "trainloaders, gini_data = \\\n",
        "  generate_dataloaders(X_train_combined, y_train_combined,\n",
        "                       method_selected, n_splits)\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).long())\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "gini_df = pd.DataFrame(gini_data)\n",
        "print(gini_df)\n",
        "\n",
        "print(\"Number of Training Subsets: \", len(trainloaders))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTaN5hUoig6k"
      },
      "outputs": [],
      "source": [
        "xdf = plot_classes(trainloaders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cpk9hkPDMiZ"
      },
      "source": [
        "##Compute weights\n",
        "\n",
        "- Index $i$ runs on clients, i.e, 1 to 8 clients\n",
        "- Index $j$ runs on classes, i.e, 1 to 6 classes\n",
        "- Properties on $i$ are related to clients: weights $w_i$, Gini coefficient $G_i$, entropy $H_i$\n",
        "- Properties on $j$ are related to classes: probabilities for picking particular class $j$ for client $i$.\n",
        "\n",
        "A vector of probabilities $p_i$ is local for a client $i$: $\\vec{p}_i=p^i_j\\rightarrow \\{p^i_1, \\dots, p^i_6\\}$ with $p^i_j = \\frac{\\text{count class}\\, j\\, \\text{client}\\, i}{N_i}$ with $N_i$ the total number of samples for client $i$. $p^i_j$ is normalized.\n",
        "\n",
        "\n",
        "The vector $P_j$ is global (common to all clients) $\\vec{P}=P_j\\rightarrow \\{P_1, \\dots, P_6\\}$ with $P_j = \\frac{\\text{count class j for all clients}}{N}$ with $N =\\sum_i N_i$ the total number of samples for all clients. $P_j$ is normalized.\n",
        "\n",
        "A set of weights could be constructed by multiplying a vector of a local property times a set of local vectors for each client. For instance, let $\\tilde{P}$ be the normalized product of the probabilities of picking a class $j$ in the joint dataset times the square probability for a client $i$ to have this class $j$,\n",
        "$$\\tilde{P}_i = \\frac{\\sum_j P_j \\cdot (p^j_i)^2}{\\sum_{i}\\sum_{j} P_j \\cdot (p^j_i)^2}$$\n",
        "\n",
        "Setting this squared probabilities adds more importance to majority classes respect to the minority classes.\n",
        "Then the normalized weights can be computed as the inverse of this value:\n",
        "$$w_i = \\frac{1/\\tilde{P_i}}{\\sum_i 1/\\tilde{P}_i}$$\n",
        "\n",
        "\n",
        "We can weight by sample size $N_i$ per client $i$ as follows,\n",
        "$$\\alpha_i = \\frac{N_i}{N}$$\n",
        "\n",
        "Finally, the weights are,\n",
        "$$W_i = \\frac{\\alpha_i w_i}{\\sum_i \\alpha_i w_i}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcn9fAU19eMv"
      },
      "outputs": [],
      "source": [
        "# group the dataset by client and class and add the samples\n",
        "nxdf = xdf.groupby([\"client\", \"class\"]).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Q0XlEcCEOg"
      },
      "outputs": [],
      "source": [
        "# build the matrix of number of samples per client per class\n",
        "S = nxdf.to_numpy().reshape((n_clients, n_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCqPFFYBEdTl"
      },
      "outputs": [],
      "source": [
        "def compute_weights(s):\n",
        "  \"\"\"Compute weights given the count of samples per client per class s\n",
        "  \"\"\"\n",
        "  print(\"Count of samples per class per client\\n\", s)\n",
        "\n",
        "  # compute alpha ratio of sample sizes for all clients\n",
        "  alpha = np.sum(s, axis=1)/np.sum(s)\n",
        "\n",
        "  # compute pij probabilities for client i to have j class\n",
        "  p = np.zeros((n_clients, n_classes))\n",
        "  for i in np.arange(n_clients):\n",
        "    p[i] = s[i] / np.sum(s[i])\n",
        "  print(\"\\nProbabilities per client per class\\n\", p)\n",
        "\n",
        "  # Compute the probabilities of having class j if we merge all datasets\n",
        "  S = np.zeros(n_classes)\n",
        "  P = np.zeros(n_classes)\n",
        "  for j, ss in enumerate(s.T):\n",
        "    S[j] = np.sum(ss)\n",
        "    P[j] = np.sum(ss) / np.sum(s)\n",
        "\n",
        "  H = [entropy(ss, base=2) for ss in s]\n",
        "  print(\"\\nTotal samples per class\", S)\n",
        "  print(\"Total probabilities per class\", P)\n",
        "  print(\"Entropy\", H)\n",
        "  print(\"Balance\", [h/np.log2(n_classes) for h in H])\n",
        "\n",
        "  # compute squared probabilities pij*pij\n",
        "  p2 = p*p\n",
        "  # compute probabilities by total probabilities P\n",
        "  iP = np.sum(p2.dot(P))/(p2.dot(P))\n",
        "  G = [1-np.sum(pp) for pp in p2]\n",
        "  print(\"Gini index \", G)\n",
        "  print(\"Mixed probabilities inverse\", iP)\n",
        "\n",
        "  # Weights will be the inverse of the result above, normalized\n",
        "  w = iP / np.sum(iP)\n",
        "  print(\"\\nPlain weights\", w)\n",
        "\n",
        "  print(\"\\nSample size ratios\", alpha)\n",
        "\n",
        "  # Final weights taking into account sample size\n",
        "  W = alpha * w / np.sum(alpha * w)\n",
        "  print(\"\\nFinal weights\", W)\n",
        "\n",
        "  # return weights and probabilities pij\n",
        "  return W, w, p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpa6hnO3E1nZ"
      },
      "outputs": [],
      "source": [
        "W, Uw, p = compute_weights(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBmTsXuGFZRM"
      },
      "outputs": [],
      "source": [
        "def plot_weights(W, p):\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "  ax2.set_title('Weights')\n",
        "  ax1.set_title(\"Probabilities per client per class $p_{ij}$\")\n",
        "  pd.DataFrame(W).plot.bar(ax=ax2, legend=False)\n",
        "  sns.heatmap(p.T, ax=ax1, annot=True, fmt=\".2f\", linewidths=.5)\n",
        "  ax1.set(xlabel='clients', ylabel='classes')\n",
        "  ax1.set_yticklabels(class_names.values())\n",
        "  ax1.set_xlabel(\"Client #\")\n",
        "  ax2.set(xlabel='clients', ylabel='weights')\n",
        "  ax2.bar_label(ax2.containers[0], fmt='%.3f')\n",
        "  ax2.set_xlabel(\"Client #\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4eS4AkLFaQ7"
      },
      "outputs": [],
      "source": [
        "plot_weights(W, p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD91sIrBEQS"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP6FFTRKBBzt"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" Multi Layer Perceptron \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        #self.flatten = nn.Flatten()\n",
        "        if shallow_model:\n",
        "          self.linear_relu_stack = nn.Sequential(\n",
        "              nn.Linear(561, 432),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(432, 6),\n",
        "          )\n",
        "          pass\n",
        "        else:\n",
        "          self.linear_relu_stack = nn.Sequential(\n",
        "              nn.Linear(561, 437),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(437, 312),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(312, 6)\n",
        "          )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "Net = MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSAcBk-Wz37t"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNebBoMmixaR"
      },
      "source": [
        "###Parameter updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JS7mbiACzyq"
      },
      "outputs": [],
      "source": [
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXUpy6XEmnXV"
      },
      "source": [
        "###Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYr_By50z37u"
      },
      "outputs": [],
      "source": [
        "def train(net, trainloader, epochs: int):\n",
        "    print(\"Round of training started\")\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    training_size = len(trainloader.dataset)\n",
        "    batch_size = trainloader.batch_size\n",
        "\n",
        "    # Modify target_epsilon and target_delta here\n",
        "    noise_generator = torch.Generator()\n",
        "    noise_generator.manual_seed(random_seed)\n",
        "\n",
        "    target_delta = 1e-5\n",
        "\n",
        "    max_grad_norm = 1.0\n",
        "    noise_multiplier = 1.0  # This value will be used to initialize the PrivacyEngine, but it will be modified automatically to reach the target epsilon\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    model = net\n",
        "    dataloader = trainloader  # Define dataloader here\n",
        "\n",
        "    if USE_DP:\n",
        "        privacy_engine = PrivacyEngine(accountant = 'rdp')\n",
        "\n",
        "        model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(\n",
        "            module=model,\n",
        "            optimizer=optimizer,\n",
        "            data_loader=dataloader,\n",
        "            target_epsilon=target_epsilon,\n",
        "            target_delta=target_delta,\n",
        "            epochs=epochs,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            noise_generator=noise_generator\n",
        "        )\n",
        "    else:\n",
        "        # If not using DP, PrivacyEngine is not defined and can't be used to get epsilon later.\n",
        "        privacy_engine = None\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "\n",
        "        epoch_loss /= len(dataloader.dataset)\n",
        "\n",
        "    # After training, you can get the final epsilon\n",
        "    if privacy_engine:  # Only try to get epsilon if privacy_engine was defined\n",
        "        final_epsilon = privacy_engine.get_epsilon(delta=target_delta)\n",
        "        print(f\"The target epsilon was: {target_epsilon}\")\n",
        "        print(f\"The final epsilon is: {final_epsilon}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKu2WdJwB0TL"
      },
      "source": [
        "##Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKSlyuX2ByI7"
      },
      "outputs": [],
      "source": [
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_predicted = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_predicted.append(predicted.cpu())\n",
        "\n",
        "    all_labels = torch.cat(all_labels) # concatenate all labels tensors\n",
        "    all_predicted = torch.cat(all_predicted) # concatenate all predicted tensors\n",
        "\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Calculate F1 score. Need to convert tensors to numpy arrays\n",
        "    f1_score_value_micro = f1_score(all_labels.numpy(), all_predicted.numpy(), average='micro')\n",
        "    f1_score_value_macro = f1_score(all_labels.numpy(), all_predicted.numpy(), average='macro')\n",
        "    f1_score_value_perclass = f1_score(all_labels.numpy(), all_predicted.numpy(), average=None)\n",
        "\n",
        "    return accuracy, loss, f1_score_value_micro, f1_score_value_macro, f1_score_value_perclass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mEMZEQz37v"
      },
      "source": [
        "#Client implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elxhgUFIz37y"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "\n",
        "    def __init__(self, cid, net, trainloader, valloader):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=n_epochs)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        accuracy, loss, f1_score_value_micro, f1_score_value_macro, f1_score_value_perclass = test(self.net, self.valloader)\n",
        "        print(f\"[Client {self.cid}] loss: {loss}, accuracy: {accuracy}, f1_score_micro: {f1_score_value_micro}, f1_score_macro: {f1_score_value_macro}, f1_score_perclass: {f1_score_value_perclass}\")  # Add this line\n",
        "\n",
        "        return float(loss), len(self.valloader), {\n",
        "                                                    \"accuracy\": float(accuracy),\n",
        "                                                    \"f1_score_micro\": float(f1_score_value_micro),\n",
        "                                                    \"f1_score_macro\": float(f1_score_value_macro),\n",
        "                                                    \"f1_score_perclass\": [float(score) for score in f1_score_value_perclass]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HARClient(fl.client.NumPyClient):\n",
        "    \"\"\"Flower client implementing for HAR data using PyTorch.\n",
        "\n",
        "    Client implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        model: object,  # har.NeuralNetwork,\n",
        "        trainloader: torch.utils.data.DataLoader,\n",
        "        valloader: torch.utils.data.DataLoader,\n",
        "        keygen,\n",
        "        debug: bool = False,\n",
        "        #test_set_name: str = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Set model and train-test data loaders.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        cid\n",
        "            Client ID\n",
        "        model\n",
        "            Torch model\n",
        "        trainloader\n",
        "            DataLoader for train dataset\n",
        "        testloader\n",
        "            DataLoader for test dataset\n",
        "        debug : bool\n",
        "            Flag for trigger some debug messages\n",
        "        \"\"\"\n",
        "        self.cid = cid\n",
        "        self.model = model\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        #self.testloader = testloader\n",
        "        # HACK this specifies if False is the first communication between\n",
        "        # server and client in a round of training\n",
        "        self.train_state = False\n",
        "        self.first_evaluation_call = True\n",
        "        self.debug = debug\n",
        "        self.keygen = keygen#sp.KeyGenerator.load()\n",
        "        #self.test_set_name =\\\n",
        "        #    NSO(int(str(os.path.basename(test_set_name)).split(\"_\")[0]))\n",
        "\n",
        "    def get_parameters(self, config=None) -> List[np.ndarray]:\n",
        "        \"\"\"Get parameters.\"\"\"\n",
        "        self.model.train()\n",
        "        print(f\"Calling get_parameters from {self.cid}\")\n",
        "        encrypted_parameters = []\n",
        "        for n, (name, val) in enumerate(self.model.state_dict().items()):\n",
        "            if n in encrypted_layers_ids:\n",
        "                print(f\"Encrypting {name} {val.cpu().numpy().shape}\")\n",
        "                enc_ndarray = (\n",
        "                    sp.EncArray(val.cpu().numpy())\n",
        "                    .encrypt_singlethread(self.keygen.public_key)\n",
        "                    .serialize_ndarray()\n",
        "                )\n",
        "                print(f\"Encrypted ndarray shape {enc_ndarray.shape}\")\n",
        "                encrypted_parameters.append(enc_ndarray)\n",
        "            else:\n",
        "                print(f\"Not encrypted {name} {val.cpu().numpy().shape}\")\n",
        "                encrypted_parameters.append(val.cpu().numpy())\n",
        "        print(f\"End calling get_parameters from {self.cid}\")\n",
        "        return encrypted_parameters\n",
        "\n",
        "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
        "        \"\"\"Set parameters.\"\"\"\n",
        "        print(f\"Setting {len(parameters)} parameters from {self.cid}\")\n",
        "        parameters_clear = []\n",
        "        # iterate in list of arrays from each client\n",
        "        for n, e in enumerate(parameters):\n",
        "            if e.flatten().dtype.type is np.str_:\n",
        "                print(\n",
        "                    f\"Deserialiazing and decrypting {e.shape} elements \"\n",
        "                )\n",
        "                # HACK exponent has changed from 32 to 47\n",
        "                enc_array = sp.EncArray.deserialize_ndarray(\n",
        "                    e, self.keygen.public_key, -47\n",
        "                )\n",
        "                print(f\"with shape {enc_array.shape}\")\n",
        "                parameters_clear.append(enc_array.decrypt_singlethread(self.keygen.private_key))\n",
        "            else:\n",
        "                # this one is on the clear\n",
        "                parameters_clear.append(e)\n",
        "        # Set model parameters from a list of NumPy ndarrays\n",
        "        self.model.train()\n",
        "        print(\"Updating model dict\")\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters_clear)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(\n",
        "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
        "    ) -> Tuple[List[np.ndarray], int]:\n",
        "        \"\"\"Set model parameters, train model, return updated model parameters.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        parameters\n",
        "            model parameters as a list of NumPy ndarrays, excluding\n",
        "            parameters of BN layers when using FedBN\n",
        "        config\n",
        "            complete\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            updated parameters, size of train dataset, None\n",
        "        \"\"\"\n",
        "        print(f\"Calling fit {config}\")\n",
        "        self.set_parameters(parameters)\n",
        "        #train(self.model, self.trainloader, epochs=n_epochs, device=DEVICE)\n",
        "        train(self.model, self.trainloader, epochs=n_epochs)\n",
        "\n",
        "        return self.get_parameters(), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(\n",
        "        self, parameters: List[np.ndarray], config: Dict[str, str]\n",
        "    ) -> Tuple[int, float, float]:\n",
        "        \"\"\"Set model parameters, evaluate model on local test dataset,\n",
        "        and return result.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        parameters\n",
        "            model parameters as a list of NumPy ndarrays, excluding\n",
        "            parameters of BN layers when using FedBN\n",
        "\n",
        "        config\n",
        "            complete.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            loss, size, and accuracy\n",
        "        \"\"\"\n",
        "        print(\"Calling evaluate\")\n",
        "\n",
        "        for n, e in enumerate(parameters):\n",
        "            if isinstance(e[0], np.str_):\n",
        "                # if need to check for ciphertexts\n",
        "                print(\"Found encrypted layers\")\n",
        "                break\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        #return float(loss), len(self.testloader), {\"accuracy\": float(accuracy)}\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        #set_parameters(self.net, parameters)\n",
        "        accuracy, loss, f1_score_value_micro, f1_score_value_macro, f1_score_value_perclass = test(self.model, self.valloader)\n",
        "        print(f\"[Client {self.cid}] loss: {loss}, accuracy: {accuracy}, f1_score_micro: {f1_score_value_micro}, f1_score_macro: {f1_score_value_macro}, f1_score_perclass: {f1_score_value_perclass}\")  # Add this line\n",
        "\n",
        "        return float(loss), len(self.valloader), {\n",
        "                                                    \"accuracy\": float(accuracy),\n",
        "                                                    \"f1_score_micro\": float(f1_score_value_micro),\n",
        "                                                    \"f1_score_macro\": float(f1_score_value_macro),\n",
        "                                                    \"f1_score_perclass\": [float(score) for score in f1_score_value_perclass]}"
      ],
      "metadata": {
        "id": "aAGoH5t8UGDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWoMdQIYcDp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create keys"
      ],
      "metadata": {
        "id": "9XvWcZGWcXEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keygen = sp.KeyGenerator()"
      ],
      "metadata": {
        "id": "31tobN4XMQrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encrypted parameters' (weights') functions"
      ],
      "metadata": {
        "id": "f_Gxa_hgMZwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eparameters(model, keygen) -> List[np.ndarray]:\n",
        "    \"\"\"Get parameters.\"\"\"\n",
        "    model.eval()\n",
        "    print(\"Calling get_eparameters\")\n",
        "    encrypted_parameters = []\n",
        "    for n, (name, val) in enumerate(model.state_dict().items()):\n",
        "        if n in encrypted_layers_ids:\n",
        "            print(f\"Encrypting {name} {val.cpu().numpy().shape}\")\n",
        "            enc_ndarray = (\n",
        "                sp.EncArray(val.cpu().numpy())\n",
        "                .encrypt_singlethread(keygen.public_key)\n",
        "                .serialize_ndarray()\n",
        "            )\n",
        "            print(f\"Encrypted ndarray shape {enc_ndarray.shape}\")\n",
        "            encrypted_parameters.append(enc_ndarray)\n",
        "        else:\n",
        "            print(f\"Not encrypted {name} {val.cpu().numpy().shape}\")\n",
        "            encrypted_parameters.append(val.cpu().numpy())\n",
        "    return encrypted_parameters\n",
        "\n",
        "def set_eparameters(parameters: List[np.ndarray]) -> None:\n",
        "    \"\"\"Set parameters.\"\"\"\n",
        "    print(f\"Setting {len(parameters)} parameters from \")\n",
        "    parameters_clear = []\n",
        "    print(keygen.public_key)\n",
        "    # iterate in list of arrays from each client\n",
        "    for n, e in enumerate(parameters):\n",
        "        if e.flatten().dtype.type is np.str_:\n",
        "            print(\n",
        "                f\"Deserialiazing and decrypting {e.shape} elements \"\n",
        "            )\n",
        "            # HACK exponent has changed from 32 to 47\n",
        "            enc_array = sp.EncArray.deserialize_ndarray(\n",
        "                e, keygen.public_key, -47\n",
        "            )\n",
        "            print(f\"with shape {enc_array.shape}\")\n",
        "            parameters_clear.append(enc_array.decrypt_singlethread(keygen.private_key))\n",
        "        else:\n",
        "            # this one is on the clear\n",
        "            parameters_clear.append(e)\n",
        "    return parameters_clear"
      ],
      "metadata": {
        "id": "0dg3wgLsMTSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encrypting and decrypting time"
      ],
      "metadata": {
        "id": "M05OlWzUNF7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "enc_time = %timeit -n5 -r1 -o eparams = get_eparameters(MLP(), keygen)"
      ],
      "metadata": {
        "id": "T7EgOvp8M7c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eparams = get_eparameters(MLP(), keygen)"
      ],
      "metadata": {
        "id": "tnbKBZdBNgK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_time = %timeit -n5 -r1 -o oparams = set_eparameters(eparams)"
      ],
      "metadata": {
        "id": "kJT2HUn1NWet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_time, dec_time"
      ],
      "metadata": {
        "id": "XEc3zPqS8evn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps_time = %timeit -n5 -r1 -o params = get_parameters(MLP())"
      ],
      "metadata": {
        "id": "ZYyRuZYABCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = get_parameters(MLP())"
      ],
      "metadata": {
        "id": "-1pwFCyICgzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_time = %timeit -n5 -r1 -o oparams = set_parameters(MLP(), params)"
      ],
      "metadata": {
        "id": "3JeCvjPKBI_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Size of encrypted parameters"
      ],
      "metadata": {
        "id": "ccFiO-MVNqqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "esize = asizeof.asizeof(eparams)\n",
        "print(esize)"
      ],
      "metadata": {
        "id": "V-jF2x9cNaJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plaintext size\n",
        "psize = asizeof.asizeof(get_parameters(MLP()))\n",
        "print(psize)"
      ],
      "metadata": {
        "id": "-qjaD9yd8DCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Client instantiation"
      ],
      "metadata": {
        "id": "GRsXglsmN93A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Client for encrypted weights"
      ],
      "metadata": {
        "id": "wR2KH7EXm5R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eclient_fn(cid) -> HARClient:\n",
        "    torch.manual_seed(1)\n",
        "    #torch.use_deterministic_algorithms(True)\n",
        "    net = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    #return FlowerClient(cid, net, trainloader, valloader)\\\n",
        "    print(keygen.public_key)\n",
        "    return HARClient(cid, net, trainloader, test_dataloader, keygen=deepcopy(keygen))"
      ],
      "metadata": {
        "id": "TMj8m7mlN__9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Client for plaintext weights"
      ],
      "metadata": {
        "id": "vprw95dDnBrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oclient_fn(cid) -> FlowerClient:\n",
        "    torch.manual_seed(1)\n",
        "    #torch.use_deterministic_algorithms(True)\n",
        "    net = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    return FlowerClient(cid, net, trainloader, test_dataloader)"
      ],
      "metadata": {
        "id": "ouEDLQXOOAbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKKveUJ2ny9"
      },
      "source": [
        "#Aggregation strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH8Tq0l_XCP_"
      },
      "source": [
        "##Custom aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JlvSFViXEzl"
      },
      "outputs": [],
      "source": [
        "class WeightedFedAvg(fl.server.strategy.FedAvg):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        fraction_fit: float = 1.0,\n",
        "        fraction_evaluate: float = 1.0,\n",
        "        min_fit_clients: int = 2,\n",
        "        min_evaluate_clients: int = 2,\n",
        "        min_available_clients: int = 2,\n",
        "        evaluate_fn: Optional[\n",
        "            Callable[\n",
        "                [int, NDArrays, Dict[str, Scalar]],\n",
        "                Optional[Tuple[float, Dict[str, Scalar]]],\n",
        "            ]\n",
        "        ] = None,\n",
        "        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        accept_failures: bool = True,\n",
        "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        keygen=None,\n",
        "        initial_parameters: Optional[Parameters] = None,\n",
        "        weighting_fn = None\n",
        "    ) -> None:\n",
        "        \"\"\"Implement Simple Paillier Averaging strategy.\n",
        "\n",
        "        Implementation based on flower FedAvg\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fraction_fit\n",
        "            Fraction of clients used during training. Defaults to 0.1.\n",
        "        fraction_evaluate\n",
        "            Fraction of clients used during validation. Defaults to 0.1.\n",
        "        min_fit_clients\n",
        "            Minimum number of clients used during training. Defaults to 2.\n",
        "        min_evaluate_clients\n",
        "            Minimum number of clients used during validation. Defaults to 2.\n",
        "        min_available_clients\n",
        "            Minimum number of total clients in the system. Defaults to 2.\n",
        "        eval_fn\n",
        "            Optional function used for validation. Defaults to None.\n",
        "        on_fit_config_fn\n",
        "            Function used to configure training. Defaults to None.\n",
        "        on_evaluate_config_fn\n",
        "            Function used to configure validation. Defaults to None.\n",
        "        accept_failures\n",
        "            Whether or not accept rounds containing failures. Defaults to True.\n",
        "        initial_parameters\n",
        "            Initial global model parameters.\n",
        "        weighting_fn\n",
        "            Custom weighting function for average aggregation of parameters\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            fraction_fit=fraction_fit,\n",
        "            fraction_evaluate=fraction_evaluate,\n",
        "            min_fit_clients=min_fit_clients,\n",
        "            min_evaluate_clients=min_evaluate_clients,\n",
        "            min_available_clients=min_available_clients,\n",
        "            evaluate_fn=evaluate_fn,\n",
        "            on_fit_config_fn=on_fit_config_fn,\n",
        "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "            accept_failures=accept_failures,\n",
        "            initial_parameters=initial_parameters,\n",
        "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        )\n",
        "        self.weighting_fn = weighting_fn\n",
        "\n",
        "    def aggregate_fit(\n",
        "          self,\n",
        "          server_round: int,\n",
        "          results: List[Tuple[ClientProxy, FitRes]],\n",
        "          failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "        ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
        "        if not results:\n",
        "            return None, {}\n",
        "        # Do not aggregate if there are failures and failures are not accepted\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "\n",
        "        # Convert results into weights\n",
        "        weights_results = [\n",
        "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
        "            for _, fit_res in results\n",
        "        ]\n",
        "        # Extract client ids\n",
        "        cids = [int(cp.cid) for cp, _ in results]\n",
        "        print(\"client proxies\", cids)\n",
        "\n",
        "        # compute weights using the weighting function\n",
        "        avg_weights = self.weighting_fn(weights_results, cids)\n",
        "\n",
        "        # Create a list of weights, each multiplied by the weights computed above\n",
        "        weighted_weights = [\n",
        "            [layer * avg_weights for layer in weights] for (weights, _), avg_weights in zip(weights_results, avg_weights)\n",
        "        ]\n",
        "\n",
        "        # Compute average weights of each layer\n",
        "        weights_prime: NDArrays = [\n",
        "            reduce(np.add, layer_updates)\n",
        "            for layer_updates in zip(*weighted_weights)\n",
        "        ]\n",
        "        return ndarrays_to_parameters(weights_prime), {}\n",
        "\n",
        "def custom_weighting(results, cids):\n",
        "    \"\"\"Given results (which have the weights) and client ids compute the vector\n",
        "    of weights of size n_clients.\n",
        "    \"\"\"\n",
        "\n",
        "    if avg_strategy == \"standard\":\n",
        "      total_samples = sum([num_samples for _, num_samples in results])\n",
        "      # Plain FedAvg as flower (no different as flower implementation)\n",
        "      weights = [num_samples / total_samples for _, num_samples in results]\n",
        "    elif avg_strategy == \"arithmetic\":\n",
        "      weights = np.ones(n_clients)/len(cids)   # Simple FedAvs, arithmetic average\n",
        "    elif avg_strategy == \"weighted\":\n",
        "      # use custom weights\n",
        "      # alpha = np.array([num_samples / total_samples for _, num_samples in results])\n",
        "      # reorder Uw that depends on (Pj, pij) and normalize by sample size alpha\n",
        "      weights = W[cids]#alpha * Uw[cids] / np.sum(alpha * Uw[cids])\n",
        "\n",
        "    #print(f\"num samples {[num_samples for _, num_samples in results]}\")\n",
        "    #print(f\"Aggregation {avg_strategy} weights {weights}\")\n",
        "    #print(f\"Sum of weights: {np.sum(weights)}\")\n",
        "    #print(f\"FedAvg weights {[num_samples / total_samples for _, num_samples in results]}\")\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encrypted custom aggregation"
      ],
      "metadata": {
        "id": "a8uzohppPlpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "enc_weights = None\n",
        "if avg_strategy == \"weighted\":\n",
        "  enc_weights = (sp.EncArray(W).encrypt_singlethread(keygen.public_key))\n",
        "  print(f\"Encrypted ndarray shape {enc_weights.shape}\")\n",
        "print(enc_weights)"
      ],
      "metadata": {
        "id": "LMhmetsjUTiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncryptedWeightedFedAvg(fedavg.FedAvg):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        fraction_fit: float = 1.0,\n",
        "        fraction_evaluate: float = 1.0,\n",
        "        min_fit_clients: int = 2,\n",
        "        min_evaluate_clients: int = 2,\n",
        "        min_available_clients: int = 2,\n",
        "        evaluate_fn: Optional[\n",
        "            Callable[\n",
        "                [int, NDArrays, Dict[str, Scalar]],\n",
        "                Optional[Tuple[float, Dict[str, Scalar]]],\n",
        "            ]\n",
        "        ] = None,\n",
        "        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        accept_failures: bool = True,\n",
        "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        keygen=None,\n",
        "        initial_parameters: Optional[Parameters] = None,\n",
        "        weighting_fn = None\n",
        "    ) -> None:\n",
        "        \"\"\"Implement Encrypted Weighted Averaging strategy.\n",
        "\n",
        "        Implementation based on flower FedAvg\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fraction_fit\n",
        "            Fraction of clients used during training. Defaults to 0.1.\n",
        "        fraction_evaluate\n",
        "            Fraction of clients used during validation. Defaults to 0.1.\n",
        "        min_fit_clients\n",
        "            Minimum number of clients used during training. Defaults to 2.\n",
        "        min_evaluate_clients\n",
        "            Minimum number of clients used during validation. Defaults to 2.\n",
        "        min_available_clients\n",
        "            Minimum number of total clients in the system. Defaults to 2.\n",
        "        eval_fn\n",
        "            Optional function used for validation. Defaults to None.\n",
        "        on_fit_config_fn\n",
        "            Function used to configure training. Defaults to None.\n",
        "        on_evaluate_config_fn\n",
        "            Function used to configure validation. Defaults to None.\n",
        "        accept_failures\n",
        "            Whether or not accept rounds containing failures. Defaults to True.\n",
        "        keygen\n",
        "            Encryption keys\n",
        "        initial_parameters\n",
        "            Initial global model parameters.\n",
        "        weighting_fn\n",
        "            Custom weighting function for average aggregation of parameters\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            fraction_fit=fraction_fit,\n",
        "            fraction_evaluate=fraction_evaluate,\n",
        "            min_fit_clients=min_fit_clients,\n",
        "            min_evaluate_clients=min_evaluate_clients,\n",
        "            min_available_clients=min_available_clients,\n",
        "            evaluate_fn=evaluate_fn,\n",
        "            on_fit_config_fn=on_fit_config_fn,\n",
        "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "            accept_failures=accept_failures,\n",
        "            initial_parameters=initial_parameters,\n",
        "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        )\n",
        "        self.other = 1\n",
        "        print(\"config\")\n",
        "        self.keygen = keygen\n",
        "        print(self.keygen.public_key)\n",
        "        self.weighting_fn = weighting_fn\n",
        "\n",
        "    def aggregate_fit(\n",
        "          self,\n",
        "          server_round: int,\n",
        "          results: List[Tuple[ClientProxy, FitRes]],\n",
        "          failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "        ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        \"\"\"Aggregate fit results using encrypted weighted average.\"\"\"\n",
        "        if not results:\n",
        "            return None, {}\n",
        "        # Do not aggregate if there are failures and failures are not accepted\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "\n",
        "        print(\"Public key\", self.keygen.public_key)\n",
        "        # Convert results\n",
        "        res = []\n",
        "        for client, fit_res in results:\n",
        "            # bytes to ndarray\n",
        "            res_array = parameters_to_ndarrays(fit_res.parameters)\n",
        "            weights_results = []\n",
        "            # iterate in list of arrays from each client\n",
        "            for n, e in enumerate(res_array):\n",
        "                # check for encrypted serialized array\n",
        "                if e.flatten().dtype.type is np.str_:\n",
        "                    print(f\"Deserializing {e.size} elements.\")\n",
        "                    enc_array = sp.EncArray.deserialize_ndarray(e, self.keygen.public_key)\n",
        "                    print(f\"with shape {enc_array.shape}\")\n",
        "                    weights_results.append(enc_array)\n",
        "                else:\n",
        "                    # parameters on the clear, just append them\n",
        "                    weights_results.append(e)\n",
        "            res.append((weights_results, fit_res.num_examples))\n",
        "        num_examples_total = sum([num_examples for _, num_examples in res])\n",
        "\n",
        "        ## Convert results into weights\n",
        "        #weights_results = [\n",
        "        #    (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
        "        #    for _, fit_res in results\n",
        "        #]\n",
        "        # Extract client ids\n",
        "        cids = [int(cp.cid) for cp, _ in results]\n",
        "        print(\"client proxies\", cids)\n",
        "\n",
        "        # compute weights using the weighting function\n",
        "        avg_weights = self.weighting_fn(res, cids)\n",
        "\n",
        "        # Create a list of weights, each multiplied by the weights computed above\n",
        "        weighted_weights = [\n",
        "            [layer * avg_weights for layer in weights] for (weights, _), avg_weights in zip(res, avg_weights)\n",
        "        ]\n",
        "\n",
        "        # Compute average weights of each layer\n",
        "        weights_prime: NDArrays = [\n",
        "            reduce(np.add, layer_updates)\n",
        "            for layer_updates in zip(*weighted_weights)\n",
        "        ]\n",
        "\n",
        "        # serialization for transmission, we have a mix of clear & ciphertexts\n",
        "        weights_pp = []\n",
        "        for n, e in enumerate(weights_prime):\n",
        "            if isinstance(e, sp.EncArray):\n",
        "                weights_pp.append(e.serialize_ndarray())\n",
        "            else:\n",
        "                weights_pp.append(e)\n",
        "\n",
        "        # ndarray to bytes for transmission\n",
        "        return ndarrays_to_parameters(weights_pp), {}\n",
        "\n",
        "    def evaluate(\n",
        "        self, server_round: int, parameters: Parameters\n",
        "        ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
        "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
        "        print(\n",
        "            \"Calling evaluate... Not implemented. Server cant see the data!\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "def custom_enc_weighting(results, cids):\n",
        "    \"\"\"Given results (which have the weights) and client ids compute the vector\n",
        "    of weights of size n_clients.\n",
        "    \"\"\"\n",
        "\n",
        "    #total_samples = sum([num_samples for _, num_samples in results])\n",
        "    if avg_strategy == \"standard\":\n",
        "      total_samples = sum([num_samples for _, num_samples in results])\n",
        "      # Plain FedAvg as flower (no different as flower implementation)\n",
        "      weights = np.array([num_samples / total_samples for _, num_samples in results])\n",
        "    elif avg_strategy == \"arithmetic\":\n",
        "      weights = np.ones(n_clients)/len(cids)   # Simple FedAvs, arithmetic average\n",
        "    elif avg_strategy == \"weighted\":\n",
        "      # use custom weights\n",
        "      # alpha = np.array([num_samples / total_samples for _, num_samples in results])\n",
        "      # reorder Uw that depends on (Pj, pij) and normalize by sample size alpha\n",
        "      weights = W[cids]#enc_weights[cids]#W[cids]#alpha * Uw[cids] / np.sum(alpha * Uw[cids])\n",
        "\n",
        "    #print(f\"num samples {[num_samples for _, num_samples in results]}\")\n",
        "    #print(f\"Aggregation {avg_strategy} weights {weights}\")\n",
        "    #print(f\"Sum of weights: {np.sum(weights)}\")\n",
        "    #print(f\"FedAvg weights {[num_samples / total_samples for _, num_samples in results]}\")\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "XNwFAH_EPhoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iDly_XYdBOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntfk5rvIMB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "w1 * Enc(m1) + w2 * Enc(2)\n",
        "Enc(w1) * Enc(m1) + ..."
      ],
      "metadata": {
        "id": "yu03lrjxMDlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADMtlWBEzEhO"
      },
      "outputs": [],
      "source": [
        "# how to aggregate custom evaluation results https://flower.dev/docs/save-progress.html\n",
        "\n",
        "Scalar = NewType(\"Scalar\", float)\n",
        "model_metrics = {}\n",
        "set_random_seeds(random_seed)\n",
        "\n",
        "def custom_aggregate_evaluate(\n",
        "        rnd: int,\n",
        "        results: List[Tuple[fl.server.client_proxy.ClientProxy , fl.common.EvaluateRes]],\n",
        "        failures: List[BaseException],\n",
        "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
        "\n",
        "    if not results:\n",
        "        return None, {}\n",
        "\n",
        "    weights, losses, metrics = zip(*[(r.num_examples, r.loss, r.metrics) for _, r in results])\n",
        "\n",
        "    total_weight = sum(weights)\n",
        "\n",
        "    weighted_loss_sum = sum(w * l for w, l in zip(weights, losses))\n",
        "\n",
        "    loss = weighted_loss_sum / total_weight\n",
        "\n",
        "    aggregated_metrics = {}\n",
        "    num_classes = len(results[0][1].metrics.get('f1_score_perclass', []))\n",
        "\n",
        "    for metric_name in results[0][1].metrics:\n",
        "        if metric_name != \"f1_score_perclass\":\n",
        "            metric_sum = sum(\n",
        "                r.metrics.get(metric_name, 0) * r.num_examples for _, r in results\n",
        "            )\n",
        "            aggregated_metric = metric_sum / total_weight\n",
        "            aggregated_metrics[metric_name] = aggregated_metric\n",
        "        else:\n",
        "            # Calculate weighted F1 scores for each class\n",
        "            per_class_f1_scores = [r.metrics.get(metric_name, [0]*num_classes) for _, r in results]\n",
        "            per_class_f1_scores_weighted_sum = [\n",
        "                sum(w * class_f1_score for w, class_f1_score in zip(weights, class_f1_scores))\n",
        "                for class_f1_scores in zip(*per_class_f1_scores)\n",
        "            ]\n",
        "            aggregated_per_class_f1_scores = [weighted_sum / total_weight for weighted_sum in per_class_f1_scores_weighted_sum]\n",
        "            aggregated_metrics[metric_name] = aggregated_per_class_f1_scores\n",
        "\n",
        "    # Return aggregated loss and metrics (i.e., aggregated accuracy and F1 score)\n",
        "    return loss, aggregated_metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiment"
      ],
      "metadata": {
        "id": "TxOE988rYvFf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbAyv8V5albx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "strategies_acc = []\n",
        "time_acc = {}\n",
        "model_metricsF1={}\n",
        "\n",
        "loop_list = strategy_list\n",
        "\n",
        "if loop_on_strategies is False:\n",
        "  loop_list = [strategy_type]\n",
        "\n",
        "for strategy_type in loop_list:\n",
        "  print(\"\\nAnalyzing Strategy... : \", strategy_type.capitalize())\n",
        "  init_time_st = timeit.default_timer()\n",
        "\n",
        "  params = get_parameters(MLP())\n",
        "\n",
        "  if strategy_type == 'fedavg':\n",
        "    # Creazione della Strategia FedAvg\n",
        "    # Passa i parametri alla strategia per l'inizializzazione dei parametri lato Server\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1./n_clients,\n",
        "        min_fit_clients=n_clients,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=n_clients,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        "    )\n",
        "    client_fn = oclient_fn\n",
        "\n",
        "  if strategy_type == 'weightedfedavg':\n",
        "    # Creazione della Strategia personalizzata WeightedFedAvg\n",
        "\n",
        "    strategy = WeightedFedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1./n_clients,\n",
        "        min_fit_clients=n_clients,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=n_clients,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        "        weighting_fn=custom_weighting\n",
        "        )\n",
        "    client_fn = oclient_fn\n",
        "\n",
        "  if strategy_type == \"secwfedavg\":\n",
        "    strategy = EncryptedWeightedFedAvg(\n",
        "          fraction_fit=1.0,\n",
        "          fraction_evaluate=1.0/n_clients,\n",
        "          min_fit_clients=n_clients,\n",
        "          min_evaluate_clients=1,\n",
        "          min_available_clients=n_clients,\n",
        "          keygen=keygen,\n",
        "          initial_parameters=fl.common.ndarrays_to_parameters(eparams),\n",
        "          weighting_fn=custom_enc_weighting\n",
        "      )\n",
        "    # set the proper client for encrypted weights\n",
        "    client_fn = eclient_fn\n",
        "\n",
        "  if strategy_type == 'fedadagrad':\n",
        "    # Creazione della Strategia FedAdagrad\n",
        "    strategy = fl.server.strategy.FedAdagrad(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1./n_clients,\n",
        "        min_fit_clients=n_clients,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=n_clients,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        "    )\n",
        "    client_fn = oclient_fn\n",
        "\n",
        "  if strategy_type == 'fedadam':\n",
        "    # Creazione della Strategia FedAdam\n",
        "    strategy = fl.server.strategy.FedAdam(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1./n_clients,\n",
        "        min_fit_clients=n_clients,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=n_clients,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(params))\n",
        "    client_fn = oclient_fn\n",
        "\n",
        "  if strategy_type == 'fedyogi':\n",
        "    # Creazione della Strategia FedYogi\n",
        "    # Passa i parametri alla strategia per l'inizializzazione dei parametri lato Server\n",
        "    strategy = fl.server.strategy.FedYogi(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1./n_clients,\n",
        "        min_fit_clients=n_clients,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=n_clients,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        "    )\n",
        "    client_fn = oclient_fn\n",
        "\n",
        "  strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
        "\n",
        "  # Specifica le risorse del client se si ha bisogno della GPU (default a 1 per CPU e 0 per GPU)\n",
        "  client_resources = None\n",
        "  if DEVICE.type == \"cuda\":\n",
        "      client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "  # Avvio della Simulazione\n",
        "  history = fl.simulation.start_simulation(\n",
        "      client_fn=client_fn,\n",
        "      num_clients=n_clients,\n",
        "      config=fl.server.ServerConfig(num_rounds=n_rounds),\n",
        "      strategy=strategy,\n",
        "      client_resources=client_resources,\n",
        "  )\n",
        "\n",
        "  time_st = timeit.default_timer() - init_time_st\n",
        "  time_acc[strategy_type] = time_st\n",
        "  print(f\"\\nrun time for strategy {strategy_type.capitalize()} : {time_st}\")\n",
        "\n",
        "  aggregated_metrics = history.metrics_distributed\n",
        "\n",
        "  loss_dist = history.losses_distributed\n",
        "  acc = [m[1] for m in history.metrics_distributed['accuracy']]\n",
        "  f1_scores_micro = [m[1] for m in history.metrics_distributed['f1_score_micro']]\n",
        "  f1_scores_macro = [m[1] for m in history.metrics_distributed['f1_score_macro']]\n",
        "  f1_scores_perclass = [m[1] for m in history.metrics_distributed['f1_score_perclass']]\n",
        "  rounds = [m[0] for m in history.metrics_distributed['accuracy']]\n",
        "\n",
        "  model_metrics[strategy_type] = {'accuracy': acc, 'f1_score_micro': f1_scores_micro, 'f1_score_macro': f1_scores_macro, 'f1_score_perclass': f1_scores_perclass}\n",
        "  model_metricsF1[strategy_type] = {'f1_score': f1_scores_macro}\n",
        "  # Plot the accuracy and F1 scores\n",
        "  plt.figure()\n",
        "  plt.plot(rounds, acc, label=strategy_type.capitalize()+' Accuracy', color = strategy_colors[loop_list.index(strategy_type)])\n",
        "  plt.plot(rounds, f1_scores_micro, label=strategy_type.capitalize()+' F1 Score Micro', color = strategy_colors[(loop_list.index(strategy_type) + 1) % len(strategy_colors)])\n",
        "  plt.plot(rounds, f1_scores_macro, label=strategy_type.capitalize()+' F1 Score Macro', color = strategy_colors[(loop_list.index(strategy_type) + 2) % len(strategy_colors)])\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(strategy_type.capitalize() + \" Accuracy and F1 Scores\")\n",
        "  plt.xlabel(\"Round\")  # Added x-label\n",
        "  plt.ylabel(\"Score\")  # Added y-label\n",
        "  plt.savefig(f\"{save_path}/\"+strategy_type+\"_acc_f1.png\", dpi = 300)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  # Plot each class' F1 score in a new figure\n",
        "  plt.figure()\n",
        "\n",
        "  # Plot each class' F1 score in a new figure\n",
        "  plt.figure()\n",
        "  for i, class_f1_scores in enumerate(zip(*f1_scores_perclass)):\n",
        "      plt.plot(rounds, class_f1_scores, label=f' {class_names[i]}', color = strategy_colors[(loop_list.index(strategy_type) + 3 + i) % len(strategy_colors)])\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(strategy_type.capitalize() + \" Per Class F1 Scores\")\n",
        "  plt.xlabel(\"Round\")  # Added x-label\n",
        "  plt.ylabel(\"Score\")  # Added y-label\n",
        "  plt.savefig(f\"{save_path}/\"+strategy_type+\"_perclass_f1.png\", dpi = 300)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  strategies_acc.append((strategy_type, max(acc), acc[-1], max(f1_scores_micro), f1_scores_micro[-1], max(f1_scores_macro), f1_scores_macro[-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  aggregated_metrics = history.metrics_distributed\n",
        "\n",
        "  loss_dist = history.losses_distributed\n",
        "  acc = [m[1] for m in history.metrics_distributed['accuracy']]\n",
        "  f1_scores_micro = [m[1] for m in history.metrics_distributed['f1_score_micro']]\n",
        "  f1_scores_macro = [m[1] for m in history.metrics_distributed['f1_score_macro']]\n",
        "  f1_scores_perclass = [m[1] for m in history.metrics_distributed['f1_score_perclass']]\n",
        "  rounds = [m[0] for m in history.metrics_distributed['accuracy']]\n",
        "\n",
        "  model_metrics[strategy_type] = {'accuracy': acc, 'f1_score_micro': f1_scores_micro, 'f1_score_macro': f1_scores_macro, 'f1_score_perclass': f1_scores_perclass}\n",
        "\n",
        "  # Plot the accuracy and F1 scores\n",
        "  plt.figure()\n",
        "  plt.plot(rounds, acc, label=strategy_type.capitalize()+' Accuracy', color = strategy_colors[loop_list.index(strategy_type)])\n",
        "  plt.plot(rounds, f1_scores_micro, label=strategy_type.capitalize()+' F1 Score Micro', color = strategy_colors[(loop_list.index(strategy_type) + 1) % len(strategy_colors)])\n",
        "  plt.plot(rounds, f1_scores_macro, label=strategy_type.capitalize()+' F1 Score Macro', color = strategy_colors[(loop_list.index(strategy_type) + 2) % len(strategy_colors)])\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(strategy_type.capitalize() + \" Accuracy and F1 Scores\")\n",
        "  plt.xlabel(\"Round\")  # Added x-label\n",
        "  plt.ylabel(\"Score\")  # Added y-label\n",
        "  plt.savefig(f\"{save_path}/\"+strategy_type+\"_acc_f1.png\", dpi = 300)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  # Walking, Walking_upstairs, Walking_downstairs, Sitting, Standing, Laying\n",
        "  #class_names = {0: \"Walking\", 1: \"Walking_upstairs\", 2: \"Walking_downstairs\", 3: \"Sitting\", 4: \"Standing\", 5: \"Laying\"}\n",
        "\n",
        "\n",
        "  # Plot each class' F1 score in a new figure\n",
        "  plt.figure()\n",
        "\n",
        "  # Plot each class' F1 score in a new figure\n",
        "  plt.figure()\n",
        "  for i, class_f1_scores in enumerate(zip(*f1_scores_perclass)):\n",
        "      plt.plot(rounds, class_f1_scores, label=f' {class_names[i]}', color = strategy_colors[(loop_list.index(strategy_type) + 3 + i) % len(strategy_colors)])\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(strategy_type.capitalize() + \" Per Class F1 Scores\")\n",
        "  plt.xlabel(\"Round\")  # Added x-label\n",
        "  plt.ylabel(\"Score\")  # Added y-label\n",
        "  plt.savefig(f\"{save_path}/\"+strategy_type+\"_perclass_f1.png\", dpi = 300)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  strategies_acc.append((strategy_type, max(acc), acc[-1], max(f1_scores_micro), f1_scores_micro[-1], max(f1_scores_macro), f1_scores_macro[-1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "kfsBSNtg4EKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqcYMOEpIhup"
      },
      "outputs": [],
      "source": [
        "strategies_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "E1MfNZDLgLgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = mpl.color_sequences['Set3']\n",
        "\n",
        "\n",
        "# Plot F1 scores per method\n",
        "plt.figure()\n",
        "#plt.plot(rounds, acc, label=strategy_type.capitalize()+' Accuracy', color = strategy_colors[loop_list.index(strategy_type)])\n",
        "#plt.plot(rounds, f1_scores_micro, label=strategy_type.capitalize()+' F1 Score Micro', color = strategy_colors[(loop_list.index(strategy_type) + 1) % len(strategy_colors)])\n",
        "#plt.plot(rounds, f1_scores_macro, label=strategy_type.capitalize()+' F1 Score Macro', color = strategy_colors[(loop_list.index(strategy_type) + 2) % len(strategy_colors)])\n",
        "i=0\n",
        "for strategy_type in loop_list:\n",
        "  #plt.plot(rounds, model_metricsF1[strategy_type]['f1_score'] , label=strategy_type.capitalize(), marker='o', markersize=4, linestyle='-', linewidth=1, color = strategy_colors[(loop_list.index(strategy_type) + 2) % len(strategy_colors)])\n",
        "  plt.plot(rounds, model_metricsF1[strategy_type]['f1_score'] , label=strategy_type.capitalize(), marker='o', markersize=4, linestyle='-', linewidth=1, color = cmap[i])\n",
        "  i=i+1\n",
        "plt.legend()\n",
        "plt.title(method_selected.capitalize() + \" - F1 Scores\")\n",
        "\n",
        "#plt.title(strategy_type.capitalize() + \" Accuracy and F1 Scores\")\n",
        "plt.xlabel(\"Round\")  # Added x-label\n",
        "plt.ylabel(\"Score\")  # Added y-label\n",
        "#plt.savefig(f\"{save_path}/\"+strategy_type+\"_acc_f1.png\", dpi = 300)\n",
        "\n",
        "plt.savefig(f\"{save_path}/\"+method_selected+\"_f1.png\", dpi = 300)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "YEpd5pOKgGqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO4QzZ5E0F9Q"
      },
      "outputs": [],
      "source": [
        "#Save the output data in a json format.\n",
        "epsilon_str = str(target_epsilon).replace('.', '_')\n",
        "with open(f'{save_path}/model_metrics_epsilon_{epsilon_str}.json', 'w') as f:\n",
        "    json.dump(model_metrics, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rbjLf5utCQn"
      },
      "outputs": [],
      "source": [
        "# List of metrics to plot\n",
        "metrics_to_plot = ['accuracy', 'f1_score_micro', 'f1_score_macro']\n",
        "\n",
        "# Iterate over each metric\n",
        "for metric in metrics_to_plot:\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    for idx, (strategy, metrics) in enumerate(model_metrics.items()):\n",
        "        metric_scores = metrics[metric]\n",
        "        rounds = list(range(1, len(metric_scores) + 1))\n",
        "        ax.plot(rounds, metric_scores, marker='o', markersize=4, linestyle='-', linewidth=1, color=palette[idx], label='Testing ' + metric + ' - ' + strategy)\n",
        "    ax.set_ylim(bottom=0, top=1)  # Ensure the y-axis range is [0, 1]\n",
        "    ax.set_title('Testing ' + metric.capitalize(), fontsize=14)\n",
        "    ax.set_xlabel('Rounds', fontsize=12)\n",
        "    ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/fl_\" + metric + \"_over_strategies.png\", dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9BRwnQhvIN7"
      },
      "outputs": [],
      "source": [
        "# Loop through each strategy\n",
        "for strategy, metrics in model_metrics.items():\n",
        "\n",
        "    # Get F1 scores per class\n",
        "    f1_scores_perclass = metrics['f1_score_perclass']\n",
        "\n",
        "    # Assume that f1_scores_perclass is a list of lists\n",
        "    # where each sublist is a list of F1 scores for each class for a particular round\n",
        "    num_classes = len(f1_scores_perclass[0])\n",
        "    num_rounds = len(f1_scores_perclass)\n",
        "\n",
        "    # Create rounds list\n",
        "    rounds = list(range(1, num_rounds + 1))\n",
        "\n",
        "    # Create a subplot for this strategy\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # For each class\n",
        "    for class_idx in range(num_classes):\n",
        "        # Extract F1 scores for this class across all rounds\n",
        "        class_f1_scores = [round_f1_scores[class_idx] for round_f1_scores in f1_scores_perclass]\n",
        "        # Plot\n",
        "        ax.plot(rounds, class_f1_scores, marker='o', markersize=4, linestyle='-', linewidth=1, color=palette[class_idx], label=class_names[class_idx])\n",
        "\n",
        "    ax.set_ylim(bottom=0, top=1)\n",
        "    ax.set_title(f'Testing F1 Score for All Classes, Strategy: {strategy}', fontsize=14)\n",
        "    ax.set_xlabel('Rounds', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/fl_f1score_perclass_{strategy}.png\", dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each strategy\n",
        "for strategy, metrics in model_metrics.items():\n",
        "\n",
        "    # Get F1 scores per class\n",
        "    f1_scores_perclass = metrics['f1_score_perclass']\n",
        "\n",
        "    # Assume that f1_scores_perclass is a list of lists\n",
        "    # where each sublist is a list of F1 scores for each class for a particular round\n",
        "    num_classes = len(f1_scores_perclass[0])\n",
        "    num_rounds = len(f1_scores_perclass)\n",
        "\n",
        "    # Create rounds list\n",
        "    rounds = list(range(1, num_rounds + 1))\n",
        "\n",
        "    # Create a subplot for this strategy\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # For each class\n",
        "    for class_idx in range(num_classes):\n",
        "        # Extract F1 scores for this class across all rounds\n",
        "        class_f1_scores = [round_f1_scores[class_idx] for round_f1_scores in f1_scores_perclass]\n",
        "        # Plot\n",
        "        ax.plot(rounds, class_f1_scores, marker='o', markersize=4, linestyle='-', linewidth=1, color=palette[class_idx], label=class_names[class_idx])\n",
        "\n",
        "    ax.set_ylim(bottom=0, top=1)\n",
        "    ax.set_title(f'Testing F1 Score for All Classes, Strategy: {strategy}', fontsize=14)\n",
        "    ax.set_xlabel('Rounds', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/fl_f1score_perclass_{strategy}.png\", dpi=300)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hBhGzs6_gsQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23RqRY3YXyNa"
      },
      "outputs": [],
      "source": [
        "strategies_acc_df = pd.DataFrame(strategies_acc, columns=[\n",
        "    'Strategy',\n",
        "    'Max Accuracy',\n",
        "    'Last Accuracy',\n",
        "    'Max F1 Score Micro',\n",
        "    'Last F1 Score Micro',\n",
        "    'Max F1 Score Macro',\n",
        "    'Last F1 Score Macro'\n",
        "])\n",
        "\n",
        "# Create separate DataFrames for each metric\n",
        "accuracy_df = strategies_acc_df[['Strategy', 'Max Accuracy', 'Last Accuracy']].melt(id_vars='Strategy', var_name='Metric', value_name='Accuracy')\n",
        "f1_micro_df = strategies_acc_df[['Strategy', 'Max F1 Score Micro', 'Last F1 Score Micro']].melt(id_vars='Strategy', var_name='Metric', value_name='F1 Score')\n",
        "f1_macro_df = strategies_acc_df[['Strategy', 'Max F1 Score Macro', 'Last F1 Score Macro']].melt(id_vars='Strategy', var_name='Metric', value_name='F1 Score')\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Strategy', y='Accuracy', hue='Metric', data=accuracy_df, palette=palette, capsize=.1, errwidth=1)  # Added error bars\n",
        "plt.ylim(0, 1)  # Set the y-axis range to be [0, 1]\n",
        "plt.title('Comparison of Max and Last Accuracy Across Strategies')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{save_path}/fl_accuracy_bar_over_strategies.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot F1 Score Micro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Strategy', y='F1 Score', hue='Metric', data=f1_micro_df, palette=palette, capsize=.1, errwidth=1)  # Added error bars\n",
        "plt.ylim(0, 1)  # Set the y-axis range to be [0, 1]\n",
        "plt.title('Comparison of Max and Last Micro F1 Score Across Strategies')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{save_path}/fl_f1score_micro_bar_over_strategies.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot F1 Score Macro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Strategy', y='F1 Score', hue='Metric', data=f1_macro_df, palette=palette, capsize=.1, errwidth=1)  # Added error bars\n",
        "plt.ylim(0, 1)  # Set the y-axis range to be [0, 1]\n",
        "plt.title('Comparison of Max and Last Macro F1 Score Across Strategies')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{save_path}/fl_f1score_macro_bar_over_strategies.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al6IJtZ3XIWC"
      },
      "outputs": [],
      "source": [
        "bar_width = 0.2  # adjust the bar width to fit more bars\n",
        "r1 = np.arange(len(strategies_acc_df))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + 2*bar_width for x in r1]\n",
        "r4 = [x + 3*bar_width for x in r1]\n",
        "\n",
        "# Accuracy plot remains the same\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(r1, strategies_acc_df['Max Accuracy'], width=bar_width, label='Max Accuracy')\n",
        "plt.bar(r2, strategies_acc_df['Last Accuracy'], width=bar_width, label='Last Accuracy')\n",
        "plt.xlabel('Strategy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Max Accuracy and Last Accuracy for Different Strategies')\n",
        "plt.xticks([r + bar_width / 2 for r in range(len(strategies_acc_df))], strategies_acc_df['Strategy'])\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)  # Setting y-axis limit to [0, 1]\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{save_path}/fl_accuracy_bar_over_strategies.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Updated F1 score plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(r1, strategies_acc_df['Max F1 Score Micro'], width=bar_width, label='Max F1 Score Micro')\n",
        "plt.bar(r2, strategies_acc_df['Last F1 Score Micro'], width=bar_width, label='Last F1 Score Micro')\n",
        "plt.bar(r3, strategies_acc_df['Max F1 Score Macro'], width=bar_width, label='Max F1 Score Macro')\n",
        "plt.bar(r4, strategies_acc_df['Last F1 Score Macro'], width=bar_width, label='Last F1 Score Macro')\n",
        "plt.xlabel('Strategy')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('Max and Last F1 Score (Micro and Macro) for Different Strategies')\n",
        "plt.xticks([r + 1.5*bar_width for r in range(len(strategies_acc_df))], strategies_acc_df['Strategy'])\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)  # Setting y-axis limit to [0, 1]\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{save_path}/fl_f1score_bar_over_strategies.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_time = timeit.default_timer()- start_global_time\n",
        "print(f\"Elapsed time {tot_time:.2f} s {tot_time/60:.2f} min\")"
      ],
      "metadata": {
        "id": "9Vi7MppApwDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import Markdown as md"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LoRkS-LSMTD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9v65FnaGAIxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## HE Results\n",
        "md(f\"\"\"# Results {save_path}\n",
        "### **Encryption-serialization time** {enc_time} takes {enc_time.average/ps_time.average:.2f}x compared to plaintext\n",
        "### **Deserialization-decryption time** {dec_time} takes {dec_time.average/pd_time.average:.2f}x compared to plaintext\n",
        "### **Run time** {time_acc[\"secwfedavg\"]:.2f} s takes {time_acc[\"secwfedavg\"]/time_acc[\"fedavg\"]:.2f}x compared to plain fedavg, {time_acc[\"secwfedavg\"]/time_acc[\"weightedfedavg\"]:.2f}x compared to weightedfedavg\n",
        "### **Ciphertext model expansion** {esize/psize:.2f}x\"\"\")"
      ],
      "metadata": {
        "id": "nJGlQVT4-ixX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "he_results = {}\n",
        "he_results['enc_time'] = enc_time.average\n",
        "he_results['dec_time'] = enc_time.average\n",
        "he_results['ps_time'] = ps_time.average\n",
        "he_results['ps_time'] = pd_time.average\n",
        "he_results['enc_time_overhead'] = enc_time.average/ps_time.average\n",
        "he_results['dec_time_overhead'] = dec_time.average/pd_time.average\n",
        "he_results['enc_model_size'] = esize\n",
        "he_results['plaintext_model_size'] = psize\n",
        "he_results['model_expansion'] = esize/psize\n",
        "he_results['run_time'] = time_acc\n",
        "he_results['time_overhead'] = time_acc[\"secwfedavg\"]/time_acc[\"fedavg\"]\n",
        "he_results['time_overhead_weighted'] = time_acc[\"secwfedavg\"]/time_acc[\"weightedfedavg\"]"
      ],
      "metadata": {
        "id": "qjGiGlVHE8Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save HE results\n",
        "with open(f'{save_path}/he_results.json', 'w') as f:\n",
        "    json.dump(he_results, f)"
      ],
      "metadata": {
        "id": "O1G4ImgJpWjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPTqzBrtxekU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HE 2 inner layers results"
      ],
      "metadata": {
        "id": "-WiRIYO5qcQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results layers-5-rounds-15-epochs-2_6ad4f728\n",
        "- Encryption-serialization time 15.8 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 8.41x compared to plaintext\n",
        "- Deserialization-decryption time 2.54 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 1.16x compared to plaintext\n",
        "- Run time 24.79 s takes 1.07x compared to plain fedavg, 1.03x compared to weightedfedavg\n",
        "- Ciphertext model expansion 9.67x"
      ],
      "metadata": {
        "id": "bVIVR1X_Qr4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results layers-4-5-rounds-15-epochs-2_a56bf3e4\n",
        "- Encryption-serialization time 4.15 s  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 2467.06x compared to plaintext\n",
        "- Deserialization-decryption time 628 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 295.85x compared to plaintext\n",
        "- Run time 163.78 s takes 7.73x compared to plain fedavg, 6.28x compared to weightedfedavg\n",
        "- Ciphertext model expansion 2720.99x"
      ],
      "metadata": {
        "id": "6Uz_3iPC9ZUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results layers-2-4-5-rounds-15-epochs-2_13dd53d9\n",
        "- Encryption-serialization time 11min 28s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each) takes 166326.34x compared to plaintext\n",
        "- Deserialization-decryption time 1min 38s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each) takes 22691.87x compared to plaintext\n",
        "- Run time takes too much time to run, colab breaks or disconnects\n",
        "- Ciphertext model expansion 179631.19x"
      ],
      "metadata": {
        "id": "eOYJOoaMqNOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "vxMR6ylzYkpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HE shallow model 1 inner layer"
      ],
      "metadata": {
        "id": "q3mbQFsfqhqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results shallow-layers-3-rounds-15-epochs-2_c0dddb7a\n",
        "- Encryption-serialization time 15.1 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 15.50x compared to plaintext\n",
        "- Deserialization-decryption time 2.3 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 1.86x compared to plaintext\n",
        "- Run time 19.79 s takes 1.16x compared to plain fedavg, 0.99x compared to weightedfedavg\n",
        "- Ciphertext model expansion 13.93x"
      ],
      "metadata": {
        "id": "Idy8__PewzZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results shallow-layers-2-3-rounds-15-epochs-2_8085f830\n",
        "- Encryption-serialization time 5.75 s  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 5654.66x compared to plaintext\n",
        "- Deserialization-decryption time 900 ms  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 824.07x compared to plaintext\n",
        "- Run time 226.72 s takes 13.43x compared to plain fedavg, 11.91x compared to weightedfedavg\n",
        "- Ciphertext model expansion 5605.73x"
      ],
      "metadata": {
        "id": "8KZZISNA_P49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results shallow-layers-0-2-3-rounds-15-epochs-2_1d9f9186\n",
        "- Encryption-serialization time 9min 8s  0 ns per loop (mean  std. dev. of 1 run, 5 loops each) takes 547464.16x compared to plaintext\n",
        "- Deserialization-decryption time 1min 20s  0 ns per loop (mean  std. dev. of - 1 run, 5 loops each) takes 36276.55x compared to plaintext\n",
        "- Ciphertext model expansion 528441.99x"
      ],
      "metadata": {
        "id": "lbOmMk_5Yykn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r {save_path}.zip {save_path}"
      ],
      "metadata": {
        "id": "j9Xzcm9OxzuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xL8ITvKH4FQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two inner layers\n",
        "Model:\n",
        "1. input (561, 437)\n",
        "2. ReLU\n",
        "2. inner linear (437, 312)\n",
        "3. ReLU\n",
        "5. inner linear (312, 6)\n",
        "5. output (6)\n",
        "\n",
        "|Encrypted layers|Encryption/Serialization|Deserialization/Decryption|Cyphertext expansion| Runtime overhead |\n",
        "|----|----|----|----|----|\n",
        "|6|9x|1.5x|10x|1x|\n",
        "|5, 6|2,500x|300x|3,000x|7x|\n",
        "|3, 5, 6|200,000x|25,000x|180,000x|-|"
      ],
      "metadata": {
        "id": "TM7MLNsnZI6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One inner layer\n",
        "Model:\n",
        "1. input (561, 432)\n",
        "2. ReLU\n",
        "3. inner linear (432, 6)\n",
        "4. output (6)\n",
        "\n",
        "|Encrypted layers|Encryption/Serialization|Deserialization/Decryption|Cyphertext expansion| Runtime overhead |\n",
        "|----|----|----|----|----|\n",
        "|4|16x|1.9x|14x|1x|\n",
        "|3, 4|5,700x|900x|5,700x|12x|\n",
        "|1, 3, 4|600,000x|40,000x|550,000x|-|"
      ],
      "metadata": {
        "id": "qJGTqKrXbJSY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a00SOE8ebVCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}