{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCtZONh3yU4"
      },
      "source": [
        "#**Model tuning for Human Activity Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrYSzFamz37m"
      },
      "source": [
        "#Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q69zTMh2dRH"
      },
      "source": [
        "*   Pytorch and flower installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GwEkQ0Jz37n"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision opacus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "heLsXPPdwM4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qls8qd8Iz37p"
      },
      "source": [
        "##All General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-1yvWzz37p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import timeit\n",
        "import platform\n",
        "\n",
        "from collections import OrderedDict\n",
        "from hashlib import md5\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union, NewType\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.accountants.rdp import RDPAccountant"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seaborn plot settings\n",
        "sns.set_style(\"white\")\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "sns.set_context(\"paper\", font_scale=1.2)  # Increase font size"
      ],
      "metadata": {
        "id": "1GyzoTt4np3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216WwsYEXg2j"
      },
      "source": [
        "##All Machine Learning Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMhXiHTAXhbQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch import Tensor\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Tested with flower version 3.3.0 and torch version 2.0.1+cu118**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4dmbcwvkdnl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6qMm3gfaIFL"
      },
      "outputs": [],
      "source": [
        "optuna.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I44eoVAwaXNC"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIvurwkgP3d3"
      },
      "source": [
        "##Reproducibility Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1KM00k3P2CW"
      },
      "outputs": [],
      "source": [
        "# For dataloader workers\n",
        "def _init_fn(worker_id):\n",
        "    np.random.seed(int(random_seed))\n",
        "\n",
        "\n",
        "def set_random_seeds(random_seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "random_seed = 123\n",
        "set_random_seeds(random_seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQQ--21w4ImG"
      },
      "source": [
        "##All Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQRPcxbv4HEh"
      },
      "outputs": [],
      "source": [
        "experiment_params = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqbmPw1SEj47"
      },
      "outputs": [],
      "source": [
        "# @title Globals { display-mode: \"form\" }\n",
        "USE_DP = False # @param {type:\"boolean\"}\n",
        "target_epsilon = 0.3 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "experiment_params[\"USE_DP\"] = USE_DP\n",
        "if USE_DP:\n",
        "  experiment_params[\"target_epsilon\"] = target_epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4cXeVEF91j7"
      },
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2BhNjhC94dt"
      },
      "outputs": [],
      "source": [
        "# @title ### Hypers { display-mode: \"form\" }\n",
        "# @markdown ---\n",
        "number_of_trials = 100 # @param {type:\"slider\", min:20, max:100, step:5}\n",
        "timeout = 500 # @param {type:\"slider\", min:500, max:10000, step:500}\n",
        "# @markdown ---\n",
        "n_epochs = 15 # @param {type:\"slider\", min:1, max:25}\n",
        "batch_size = 32 # @param {type:\"slider\", min:32, max:128, step:32}\n",
        "test_split_size = 0.2 # @param {type:\"slider\", min:0.1, max:0.5}\n",
        "# @markdown ---\n",
        "learning_rate_start = .00001 # @param {type:\"number\"}\n",
        "learning_rate_end = 0.01 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "max_number_inner_layers = 2 # @param {type:\"slider\", min:1, max:5, step:1}\n",
        "min_inner_size = 64 # @param {type:\"slider\", min:64, max:512, step:32}\n",
        "max_inner_size = 512 # @param {type:\"slider\", min:64, max:512, step:32}\n",
        "# @markdown ---\n",
        "#\n",
        "experiment_params[\"number_of_trials\"] = number_of_trials\n",
        "experiment_params[\"timeout\"] = timeout\n",
        "#\n",
        "experiment_params[\"n_epochs\"] = n_epochs\n",
        "experiment_params[\"batch_size\"] = batch_size\n",
        "experiment_params[\"test_split_size\"] = test_split_size\n",
        "#\n",
        "experiment_params[\"learning_rate_start\"] = learning_rate_start\n",
        "experiment_params[\"learning_rate_end\"] = learning_rate_end\n",
        "#\n",
        "experiment_params[\"max_number_inner_layers\"] = max_number_inner_layers\n",
        "experiment_params[\"min_inner_size\"] = min_inner_size\n",
        "experiment_params[\"max_inner_size\"] = max_inner_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RMI1impWqO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p9VHdJ-2-gu"
      },
      "source": [
        "##Initializations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_json = json.dumps(experiment_params)"
      ],
      "metadata": {
        "id": "CTgO_BINiU-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save path\n",
        "save_path = md5(experiment_json.encode()).hexdigest()[:8]\n",
        "print(save_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UHsmZIx8i7Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{save_path}_optuna.json', 'w') as f:\n",
        "    f.write(experiment_json)"
      ],
      "metadata": {
        "id": "eCXyPcfHiBGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDtG-ecZ2_Rl"
      },
      "outputs": [],
      "source": [
        "start_global_time = timeit.default_timer()\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "with open(f'{save_path}/optuna_parameters.json', 'w') as f:\n",
        "    f.write(experiment_json)\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Prova \"cuda\" per addestramento su GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Optuna {optuna.__version__}\"\n",
        ")\n",
        "\n",
        "OS = platform.system()           # Sistema Operativo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3-9cMBkbAsq"
      },
      "source": [
        "#Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmAPufD2Gz8-"
      },
      "source": [
        "##Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9EGcGI8bNeq"
      },
      "outputs": [],
      "source": [
        "def data_download(file_to_download, gdrive_code, OS, uncompress = True):\n",
        "  if not os.path.exists(file_to_download):\n",
        "    os.system('gdown --id \"'+gdrive_code+'\" --output '+file_to_download)\n",
        "    if OS == \"Linux\" and uncompress:\n",
        "        os.system('unzip -o -n \"./'+file_to_download+'\" -d '+os.path.dirname(file_to_download))\n",
        "    return True\n",
        "  else:\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj3J-tRF6ebG"
      },
      "outputs": [],
      "source": [
        "out = data_download(\"./har_datasets_fl.zip\", \"1LUjU4yvBRh6FPBlIHRCD2uf5zMH6l9tC\", OS)\n",
        "#urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\", filename=\"har-data.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3nf65Y1Fzg9"
      },
      "outputs": [],
      "source": [
        "trainloaders = []\n",
        "\n",
        "# Awful hack, when True this flips test and train datasets for a stratified\n",
        "# split ensuring that independent balanced samples are distributed in each split\n",
        "# Normal behavior flip=False\n",
        "flip = False\n",
        "def stratified_split(data, targets, n_splits, split_size=None):\n",
        "    # NOTE: We pick one stratified split => n_splits=1 because we want a\n",
        "    # balanced test set, the training part will be postprocessed\n",
        "    if not split_size:\n",
        "      df = pd.DataFrame(data)\n",
        "      data_length = len(df)\n",
        "      split_size = int(data_length / n_splits)\n",
        "      print(\"split_size\", test_size)\n",
        "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=split_size, random_state=random_seed)\n",
        "    for train_index, val_index in sss.split(data, targets):\n",
        "        if flip:\n",
        "          yield data[val_index], targets[val_index], data[train_index], targets[train_index]\n",
        "        else:\n",
        "          yield data[train_index], targets[train_index], data[val_index], targets[val_index]\n",
        "\n",
        "def gini_index(y):\n",
        "  uniques = np.unique(y+1, return_counts=True)\n",
        "  probs = uniques[1]/np.sum(uniques[1])\n",
        "  #print(uniques, probs, np.sum(probs))\n",
        "  gini_index = 1.0 - np.sum(probs ** 2)\n",
        "  return gini_index\n",
        "\n",
        "def get_data_from_path(path):\n",
        "    fold_number = os.path.basename(path).split('-')[0].strip()\n",
        "    trainset = pd.read_csv(f\"{path}/train/{fold_number}_ALL_train.csv\", delimiter=';')\n",
        "    testset = pd.read_csv(f\"{path}/test/{fold_number}_ALL_test.csv\", delimiter=';')\n",
        "    return trainset, testset\n",
        "\n",
        "def create_datasets_from_dataframe(df):\n",
        "    # Extract features from columns '0' to '560'\n",
        "    X = pd.concat([df[str(i)] for i in range(561)], axis=1).values\n",
        "    # Adjust labels in 'Y' column to start from 0\n",
        "    y = (df['Y'] - 1).values\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSoasfzhgUT"
      },
      "outputs": [],
      "source": [
        "# Let's combine the old data splits into a single dataframe\n",
        "all_data = []\n",
        "for path in [f.path for f in os.scandir('./har_datasets_fl') if f.is_dir()]:\n",
        "    train_df, test_df = get_data_from_path(path)\n",
        "    all_data.append(train_df)\n",
        "    all_data.append(test_df)\n",
        "\n",
        "combined_df = pd.concat(all_data, axis=0)\n",
        "print(f\"Total data points {len(combined_df)}\")\n",
        "\n",
        "X_all, y_all = create_datasets_from_dataframe(combined_df)\n",
        "\n",
        "# 1st stratified to get all train data and test data for (server) evaluation\n",
        "X_train_combined, y_train_combined, X_test, y_test =\\\n",
        "  next(stratified_split(X_all, y_all, n_splits=1, split_size=test_split_size))\n",
        "\n",
        "print(f\"Total train data points {len(X_train_combined)}\")\n",
        "print(f\"Total test data points {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataloaders(data, targets):\n",
        "    dataloaders = []\n",
        "    # Assuming set_random_seeds function is defined elsewhere\n",
        "    set_random_seeds(random_seed)\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_seed)\n",
        "\n",
        "    for train_index, test_index in sss.split(data, targets):\n",
        "        train_dataset = TensorDataset(torch.tensor(data[train_index]).float(), torch.tensor(targets[train_index]).long())\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_dataset = TensorDataset(torch.tensor(data[test_index]).float(), torch.tensor(targets[test_index]).long())\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "XN2Z6yVezUpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = \\\n",
        "  generate_dataloaders(X_test, y_test)"
      ],
      "metadata": {
        "id": "qThK6dMaz1TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes\n",
        "n_classes = len(np.unique(y_train_combined))\n",
        "# Assuming class_names is a dictionary mapping class numbers to class names\n",
        "class_names = {0: \"Walking\", 1: \"Walking\\nupstairs\", 2: \"Walking\\ndownstairs\", 3: \"Sitting\", 4: \"Standing\", 5: \"Laying\"}"
      ],
      "metadata": {
        "id": "3REVpi7lOiFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUG8sALzTd_z"
      },
      "outputs": [],
      "source": [
        "# plot classes distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD91sIrBEQS"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(trial):\n",
        "    # We optimize the number of layers\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, max_number_inner_layers)\n",
        "    layers = []\n",
        "    in_features = 561\n",
        "    for i in range(n_layers):\n",
        "      out_features = trial.suggest_int(\"n_units_innerlayer_{}\".format(i), min_inner_size, max_inner_size)\n",
        "      layers.append(nn.Linear(in_features, out_features))\n",
        "      layers.append(nn.ReLU())\n",
        "      in_features = out_features\n",
        "    layers.append(nn.Linear(in_features, n_classes))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "9JZojba94Hy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bigPHu8A6AMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP6FFTRKBBzt"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" Multi Layer Perceptron \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        #self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(561, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "Net = MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSAcBk-Wz37t"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNebBoMmixaR"
      },
      "source": [
        "###Parameter updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXUpy6XEmnXV"
      },
      "source": [
        "###Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYr_By50z37u"
      },
      "outputs": [],
      "source": [
        "def train(model, trainloader, epochs: int, optimizer):\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    training_size = len(trainloader.dataset)\n",
        "    batch_size = trainloader.batch_size\n",
        "\n",
        "    # Modify target_epsilon and target_delta here\n",
        "    noise_generator = torch.Generator()\n",
        "    noise_generator.manual_seed(random_seed)\n",
        "\n",
        "    target_delta = 1e-5\n",
        "\n",
        "    max_grad_norm = 1.0\n",
        "    noise_multiplier = 1.0  # This value will be used to initialize the PrivacyEngine, but it will be modified automatically to reach the target epsilon\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    #optimizer = torch.optim.Adam(net.parameters())\n",
        "\n",
        "    dataloader = trainloader  # Define dataloader here\n",
        "\n",
        "    if USE_DP:\n",
        "        privacy_engine = PrivacyEngine(accountant = 'rdp')\n",
        "\n",
        "        model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(\n",
        "            module=model,\n",
        "            optimizer=optimizer,\n",
        "            data_loader=dataloader,\n",
        "            target_epsilon=target_epsilon,\n",
        "            target_delta=target_delta,\n",
        "            epochs=epochs,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            noise_generator=noise_generator\n",
        "        )\n",
        "    else:\n",
        "        # If not using DP, PrivacyEngine is not defined and can't be used to get epsilon later.\n",
        "        privacy_engine = None\n",
        "\n",
        "    #model = model.to(DEVICE)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "\n",
        "        epoch_loss /= len(dataloader.dataset)\n",
        "\n",
        "    # After training, you can get the final epsilon\n",
        "    if privacy_engine:  # Only try to get epsilon if privacy_engine was defined\n",
        "        final_epsilon = privacy_engine.get_epsilon(delta=target_delta)\n",
        "        print(f\"The target epsilon was: {target_epsilon}\")\n",
        "        print(f\"The final epsilon is: {final_epsilon}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKu2WdJwB0TL"
      },
      "source": [
        "###Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKSlyuX2ByI7"
      },
      "outputs": [],
      "source": [
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_predicted = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_predicted.append(predicted.cpu())\n",
        "\n",
        "    all_labels = torch.cat(all_labels) # concatenate all labels tensors\n",
        "    all_predicted = torch.cat(all_predicted) # concatenate all predicted tensors\n",
        "\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Calculate F1 score. Need to convert tensors to numpy arrays\n",
        "    f1_score_value_micro = f1_score(all_labels.numpy(), all_predicted.numpy(), average='micro')\n",
        "    f1_score_value_macro = f1_score(all_labels.numpy(), all_predicted.numpy(), average='macro')\n",
        "    f1_score_value_perclass = f1_score(all_labels.numpy(), all_predicted.numpy(), average=None)\n",
        "\n",
        "    return accuracy, loss, f1_score_value_micro, f1_score_value_macro, f1_score_value_perclass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    #lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
        "    lr = trial.suggest_float(\"lr\", learning_rate_start, learning_rate_end, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    train(model, train_loader, n_epochs, optimizer)\n",
        "    accuracy, loss, f1_score_value_micro, f1_score_value_macro, f1_score_value_perclass = test(model, test_loader)\n",
        "    return accuracy, f1_score_value_macro"
      ],
      "metadata": {
        "id": "9Vi7MppApwDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "study = optuna.create_study(directions=[\"minimize\", \"maximize\"])\n",
        "study.optimize(objective, n_trials=number_of_trials, timeout=timeout)\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))"
      ],
      "metadata": {
        "id": "7YzQVxjC8zuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))"
      ],
      "metadata": {
        "id": "Ub91UujnClLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_with_highest_accuracy = max(study.best_trials, key=lambda t: t.values[0])\n",
        "print(f\"Trial with highest accuracy: \")\n",
        "print(f\"\\tnumber: {trial_with_highest_accuracy.number}\")\n",
        "print(f\"\\tparams: {trial_with_highest_accuracy.params}\")\n",
        "print(f\"\\tvalues: {trial_with_highest_accuracy.values}\")"
      ],
      "metadata": {
        "id": "gOFVywUZC9iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_with_highest_f1_macro = max(study.best_trials, key=lambda t: t.values[1])\n",
        "print(f\"Trial with highest f1_macro: \")\n",
        "print(f\"\\tnumber: {trial_with_highest_f1_macro.number}\")\n",
        "print(f\"\\tparams: {trial_with_highest_f1_macro.params}\")\n",
        "print(f\"\\tvalues: {trial_with_highest_f1_macro.values}\")"
      ],
      "metadata": {
        "id": "ncxFfipsE5pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(\n",
        "    study, target=lambda t: t.values[0], target_name=\"accuracy\"\n",
        ")"
      ],
      "metadata": {
        "id": "_aYravuPDOK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Elapsed time {timeit.default_timer()- start_global_time}\")"
      ],
      "metadata": {
        "id": "a98TSsDH85jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_params"
      ],
      "metadata": {
        "id": "vDOAXrWmaH8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this search\n",
        "\n",
        "These are the hypers to keep\n",
        "```\n",
        "{'USE_DP': False,\n",
        " 'number_of_trials': 100,\n",
        " 'timeout': 500,\n",
        " 'n_epochs': 15,\n",
        " 'batch_size': 32,\n",
        " 'test_split_size': 0.2,\n",
        " 'learning_rate_start': 1e-05,\n",
        " 'learning_rate_end': 0.01,\n",
        " 'max_number_inner_layers': 2,\n",
        " 'min_inner_size': 64,\n",
        " 'max_inner_size': 512}\n",
        "```\n",
        "\n",
        "Trial with highest f1_macro:\n",
        "```\n",
        "  number: 84\n",
        "\tparams: {'n_layers': 2, 'n_units_innerlayer_0': 437, 'n_units_innerlayer_1': 312, 'optimizer': 'Adam', 'lr': 0.0018673528886359607}\n",
        "\tvalues: [0.9635922330097088, 0.9650936723136906]\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "pW-nPjMsZ3Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trials_df = []\n",
        "for ct in complete_trials:\n",
        "  trials_df.append([ct.number, ct.values[0], ct.values[1], ct.params['n_layers']])\n"
      ],
      "metadata": {
        "id": "kTF9WxkedY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials_df = pd.DataFrame(trials_df, columns=[\"number\", \"acc\", \"f1\", \"n_layers\"])"
      ],
      "metadata": {
        "id": "YAQ20kP6gkFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials_df"
      ],
      "metadata": {
        "id": "nwBjYVPKeAeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_one_layer = trials_df[trials_df[\"n_layers\"]==1].sort_values(by=[\"f1\",\"acc\"], ascending=False)[:3]\n",
        "best_one_layer"
      ],
      "metadata": {
        "id": "-BFCyxbRfY58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(best_one_layer.index):\n",
        "  ct = complete_trials[i]\n",
        "  print(ct.params)"
      ],
      "metadata": {
        "id": "heqSmbzWh3Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best with one inner layer:\n",
        "```\n",
        "{'n_layers': 1, 'n_units_innerlayer_0': 437, 'optimizer': 'Adam', 'lr': 0.0013429456755218343}\n",
        "{'n_layers': 1, 'n_units_innerlayer_0': 249, 'optimizer': 'Adam', 'lr': 0.001183653983362325}\n",
        "{'n_layers': 1, 'n_units_innerlayer_0': 437, 'optimizer': 'Adam', 'lr': 0.00034193430144998076}\n",
        "```\n"
      ],
      "metadata": {
        "id": "oDlNJ7GTjrHs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}